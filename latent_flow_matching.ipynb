{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eisbetterthanpi/latent-flow-matching/blob/main/latent_flow_matching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOiDHI7taUKF"
      },
      "source": [
        "## setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "B8Pm-Fw6jn4A"
      },
      "outputs": [],
      "source": [
        "# @title mha me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    \"\"\"Zero out the parameters of a module and return it.\"\"\"\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, n_heads=None, d_head=8, cond_dim=None, dropout=0):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.n_heads = d_model // d_head\n",
        "        # self.d_head = d_model // n_heads\n",
        "        self.cond_dim = cond_dim\n",
        "        self.q = nn.Linear(d_model, d_model, bias=False)\n",
        "        self.kv = nn.Linear(cond_dim or d_model, 2*d_model, bias=False)\n",
        "        # self.k = nn.Sequential(nn.Dropout(dropout), nn.Linear(cond_dim, d_model, bias=False))\n",
        "        # self.lin = nn.Linear(d_model, d_model)\n",
        "        self.lin = zero_module(nn.Linear(d_model, d_model))\n",
        "        self.drop = nn.Dropout(dropout) # indp before q,k,v; after linout\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.scale = self.d_head ** -.5\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [batch, T, d_model]=[batch, h*w, c], [batch, num_tok, cond_dim], [batch,T]\n",
        "        batch = x.shape[0]\n",
        "        if self.cond_dim==None: cond=x # is self attn\n",
        "        Q = self.q(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2) # [batch, T, d_model] -> [batch, n_heads, T, d_head]\n",
        "        # K = self.k(x).unflatten(-1, (self.n_heads, self.d_head)).transpose(1, 2)\n",
        "        K, V = self.kv(cond).unflatten(-1, (self.n_heads, 2*self.d_head)).transpose(1, 2).chunk(2, dim=-1) # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        Q, K = self.rope(Q), self.rope(K)\n",
        "\n",
        "        # linear attention # Softmax(Q) @ (Softmax(K).T @ V)\n",
        "        if mask != None:\n",
        "            mask = mask[:, None, :, None] # [batch,T] -> [batch,1,T,1]\n",
        "            K, V = K.masked_fill(mask, -torch.finfo(x.dtype).max), V.masked_fill(mask, -torch.finfo(x.dtype).max)\n",
        "        Q, K = Q.softmax(dim=-1)*self.scale, K.softmax(dim=-2)\n",
        "        context = K.transpose(-2,-1) @ V # [batch, n_heads, d_head, d_head]\n",
        "        out = Q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # # (quadratic) attention # Softmax(Q @ K.T) @ V\n",
        "        # attn = Q @ K.transpose(-2,-1) * self.scale # [batch, n_heads, T] # [batch, n_heads, T, T/num_tok]\n",
        "        # if mask != None: attn = attn.masked_fill(mask[:, None, :, None], -torch.finfo(attn.dtype).max) # [batch,T]->[batch,1,T,1]\n",
        "        # attention = torch.softmax(attn, dim=-1)\n",
        "        # out = self.drop(attention) @ V # [batch, n_heads, T, d_head]\n",
        "\n",
        "        out = out.transpose(1, 2).flatten(2)\n",
        "        return self.lin(out) # [batch, T, d_model]\n",
        "\n",
        "# if self, dont pass cond_dim in init, dont pass cond in fwd\n",
        "# Softmax(Q @ K.T) @ V ~ Softmax(Q) @ Softmax(K).T @ V\n",
        "\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model, d_head, cond_dim=None, ff_dim=None, dropout=0.):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm1 = nn.RMSNorm(d_model) # LayerNorm RMSNorm\n",
        "        self.norm2 = nn.RMSNorm(d_model)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.cond_dim = cond_dim\n",
        "        self.self = MultiHeadAttention(d_model, d_head=d_head, dropout=0)\n",
        "        if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        act = nn.ReLU()\n",
        "        if ff_dim==None: ff_dim=d_model*4\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), # ReLU GELU\n",
        "            nn.RMSNorm(ff_dim), nn.Dropout(dropout), nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), act, nn.Linear(d_model, ff_dim),\n",
        "            # nn.RMSNorm(ff_dim), act, nn.Linear(ff_dim, d_model)\n",
        "            # nn.RMSNorm(d_model), nn.Linear(d_model, ff_dim), nn.ReLU(), nn.Dropout(dropout), # ReLU GELU\n",
        "            # nn.Linear(ff_dim, d_model), nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None): # [b,c,h,w], [batch, num_tok, cond_dim], [batch,T]\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1,2) # [b,h*w,c]\n",
        "        # if self.cond_dim==None: cond=None # is self attn\n",
        "        x = x + self.self(self.norm1(x))\n",
        "        if self.cond_dim!=None: x = x + self.cross(self.norm2(x), cond, mask)\n",
        "        x = x + self.ff(x)\n",
        "        return x.transpose(1,2).reshape(*bchw)\n",
        "\n",
        "\n",
        "\n",
        "# d_model=8\n",
        "# d_head=4\n",
        "# batch=4\n",
        "# h,w=5,6\n",
        "# x=torch.rand(batch,d_model,h,w)\n",
        "# cond_dim=10\n",
        "# model = AttentionBlock(d_model=d_model, d_head=d_head,cond_dim=cond_dim)\n",
        "# num_tok=1\n",
        "# cond=torch.rand(batch,num_tok,cond_dim)\n",
        "# mask=torch.rand(batch,h*w)>0.5\n",
        "# out = model(x, cond, mask)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "id": "j9wUA7Vk0Y03"
      },
      "outputs": [],
      "source": [
        "# @title rope & RotEmb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "class RoPE(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, seq_len=512, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.base = dim, base\n",
        "        theta = 1.0 / (base ** (torch.arange(0, dim, step=2) / dim))\n",
        "        pos = torch.arange(seq_len).unsqueeze(-1)\n",
        "        angles = (pos * theta)[None,...,None] # [seq_len, 1] * [dim // 2] -> [1, seq_len, dim // 2, 1]\n",
        "        self.rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1).flatten(-2).to(device) # [1, seq_len, dim // 2, 2] -> [1, seq_len, dim]\n",
        "\n",
        "    def forward(self, x): # [batch, T, dim] / [batch, T, n_heads, d_head]\n",
        "        seq_len = x.size(1)\n",
        "        if self.rot_emb.shape[0] < seq_len: self.__init__(self.dim, seq_len, self.base)\n",
        "        # print('rope fwd', x.shape, self.rot_emb.shape)\n",
        "        if x.dim()==4: return x * self.rot_emb[:,:seq_len].unsqueeze(2)\n",
        "        return x * self.rot_emb[:,:seq_len]\n",
        "\n",
        "class RotEmb(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, top=torch.pi, base=10000):\n",
        "        super().__init__()\n",
        "        self.theta = top / (base ** (torch.arange(0, dim, step=2, device=device) / dim))\n",
        "\n",
        "    def forward(self, pos): # [batch] in [0,1]\n",
        "        angles = (pos.unsqueeze(-1) * self.theta).unsqueeze(-1) # [seq_len, 1] * [dim // 2] -> [seq_len, dim // 2, 1]\n",
        "        rot_emb = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1) # [seq_len, dim // 2, 2]\n",
        "        return rot_emb.flatten(-2) # [seq_len, dim]\n",
        "\n",
        "class RoPE2D(nn.Module): # Rotary Positional Embeddings\n",
        "    def __init__(self, dim, h=224, w=224, base=10000):\n",
        "        super().__init__()\n",
        "        self.dim, self.h, self.w = dim, h, w\n",
        "        # # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "        # theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "        theta = 1. / (base**torch.linspace(0,1,dim//2)).unsqueeze(0)\n",
        "        y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "        y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "        # # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "        # self.rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "        self.rot_emb = torch.cat([x.sin(), y.sin()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "        # self.rot_emb = torch.cat([x.cos(), y.cos()], dim=-1).reshape(h, w, dim).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "\n",
        "    def forward(self, img): #\n",
        "        # batch, dim, h, w = img.shape\n",
        "        # print(img.shape)\n",
        "        hw = img.shape[1] # [b, hw, dim] / [b, hw, n_heads, d_head]\n",
        "        h=w=int(hw**.5)\n",
        "        if self.h < h or self.w < w: self.__init__(self.dim, h, w)\n",
        "        # print(self.rot_emb.shape)\n",
        "        # rot_emb = self.rot_emb[:, :h, :w].unsqueeze(0) # [1, h, w, dim]\n",
        "        rot_emb = self.rot_emb[:h, :w] # [h, w, dim]\n",
        "        # return img * rot_emb.flatten(end_dim=1).unsqueeze(0) # [b, hw, dim] * [1, hw, dim]\n",
        "        return img * rot_emb.flatten(end_dim=1)[None,:,None,:] # [b, hw, n_heads, d_head] * [1, hw, 1, dim]\n",
        "        # return img * self.rot_emb\n",
        "\n",
        "\n",
        "# def RoPE2D(dim=16, h=8, w=8, base=10000):\n",
        "#     # theta = 1. / (base ** (torch.arange(0, dim, step=4) / dim))\n",
        "#     theta = 1. / (base**torch.linspace(0,1,dim//4)).unsqueeze(0)\n",
        "#     y, x = torch.meshgrid(torch.arange(h), torch.arange(w), indexing=\"ij\") # print(y,x) # [h,w], y:row_num, x:col_num\n",
        "#     y, x = (y.reshape(-1,1) * theta).unsqueeze(-1), (x.reshape(-1,1) * theta).unsqueeze(-1) # [h*w,1]*[1,dim//4] = [h*w, dim//4, 1]\n",
        "#     rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1).flatten(-2) # [h*w, dim//4 ,4] -> [h*w, dim]\n",
        "#     # rot_emb = torch.cat([x.sin(), x.cos(), y.sin(), y.cos()], dim=-1)#.reshape(dim, h, w).to(device) # [h*w, dim//4 ,4] -> [h, w, dim]\n",
        "#     return rot_emb\n",
        "\n",
        "\n",
        "# rotemb = RotEmb(10)\n",
        "# seq_len=10\n",
        "# pos = torch.linspace(0,1,seq_len).to(device)#.unsqueeze(-1)\n",
        "# rot_emb = rotemb(pos)\n",
        "# print(rot_emb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dim=8\n",
        "# posemb = RoPE(dim, seq_len=512, base=10000)\n",
        "posemb = RoPE2D(dim, h=32, w=32, base=100)\n",
        "\n",
        "for x in posemb.rot_emb[0]:\n",
        "    print(x)"
      ],
      "metadata": {
        "id": "vtEM7L7KHri4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def precompute_freqs_cis_2d(dim, end, theta = 10000.0, scale=1.0, use_cls=False):\n",
        "    H = int( end**.5 )\n",
        "    # assert  H * H == end\n",
        "    flat_patch_pos = torch.arange(0 if not use_cls else -1, end) # N = end\n",
        "    x_pos = flat_patch_pos % H # N\n",
        "    y_pos = flat_patch_pos // H # N\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim)) # Hc/4\n",
        "    x_freqs = torch.outer(x_pos, freqs).float() # N Hc/4\n",
        "    y_freqs = torch.outer(y_pos, freqs).float() # N Hc/4\n",
        "    x_cis = torch.polar(torch.ones_like(x_freqs), x_freqs)\n",
        "    y_cis = torch.polar(torch.ones_like(y_freqs), y_freqs)\n",
        "    freqs_cis = torch.cat([x_cis.unsqueeze(dim=-1), y_cis.unsqueeze(dim=-1)], dim=-1) # N,Hc/4,2\n",
        "    freqs_cis = freqs_cis.reshape(end if not use_cls else end + 1, -1)\n",
        "    # we need to think how to implement this for multi heads.\n",
        "    # freqs_cis = torch.cat([x_cis, y_cis], dim=-1) # N, Hc/2\n",
        "    return freqs_cis\n",
        "\n",
        "dim, end = 8, 25\n",
        "o = precompute_freqs_cis_2d(dim, end)\n",
        "print(o)\n",
        "\n",
        "# import numpy as np\n",
        "# abs = torch.tensor([1, 2], dtype=torch.float64)\n",
        "# angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)\n",
        "# z = torch.polar(abs, angle)\n",
        "# z\n",
        "\n",
        "\n",
        "# polar = torch.randn(4,3,2)\n",
        "\n",
        "# r = polar[:,:,0]\n",
        "# theta = polar[:,:,1]\n",
        "\n",
        "# x = r * torch.cos(theta)\n",
        "# y = r * torch.sin(theta)\n",
        "r*theta.sin()\n",
        "# cartesian = torch.stack([x,y], axis = -1)\n"
      ],
      "metadata": {
        "id": "c3Va1bed5qbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "v1 = torch.arange(1., 5.)\n",
        "v2 = torch.arange(1., 4.)\n",
        "print(v1,v2)\n",
        "o =torch.outer(v1, v2)\n",
        "print(o)\n",
        "print(v1.unsqueeze(-1)@v2.unsqueeze(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGyYa07B_U-Z",
        "outputId": "819de264-f585-4ede-963f-82188c95b5b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 2., 3., 4.]) tensor([1., 2., 3.])\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 2.,  4.,  6.],\n",
            "        [ 3.,  6.,  9.],\n",
            "        [ 4.,  8., 12.]])\n",
            "tensor([[ 1.,  2.,  3.],\n",
            "        [ 2.,  4.,  6.],\n",
            "        [ 3.,  6.,  9.],\n",
            "        [ 4.,  8., 12.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JxGF8bYeGDA2"
      },
      "outputs": [],
      "source": [
        "# @title UIB\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class UIB(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, mult=4):\n",
        "        super().__init__()\n",
        "        act = nn.SiLU()\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential( # norm,act,conv\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, in_ch, kernel, 1, kernel//2, groups=in_ch, bias=False),\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, mult*in_ch, 1, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, mult*in_ch, kernel, 1, kernel//2, groups=mult*in_ch, bias=False),\n",
        "            nn.BatchNorm2d(mult*in_ch), act, nn.Conv2d(mult*in_ch, out_ch, 1, bias=False),\n",
        "            # nn.BatchNorm2d(mult*in_ch), act, zero_module(nn.Conv2d(mult*in_ch, out_ch, 1, bias=False)),\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.conv(x)\n",
        "\n",
        "# # in_ch, out_ch = 16,3\n",
        "# in_ch, out_ch = 3,16\n",
        "# model = UIB(in_ch, out_ch)\n",
        "# x = torch.rand(128, in_ch, 64, 64)\n",
        "# out = model(x)\n",
        "# print(out.shape)\n",
        "# # print(out)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "2nu4Dzma_cD5"
      },
      "outputs": [],
      "source": [
        "# @title ResBlock\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        # self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "        # self.block = nn.Sequential( # best?\n",
        "        #     nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), act,\n",
        "        #     zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)), nn.BatchNorm2d(out_ch), act,\n",
        "        #     )\n",
        "        # self.block = nn.Sequential(\n",
        "        self.block = Seq(\n",
        "            nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), act, zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "            nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        return self.block(x, emb) + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "\n",
        "# class Res(nn.Module):\n",
        "#     def __init__(self, model):\n",
        "#         super().__init__()\n",
        "#         self.model = model\n",
        "#     def forward(self, x): return x + self.model(x)\n",
        "\n",
        "\n",
        "# import inspect\n",
        "# class Seq(nn.Sequential):\n",
        "#     def __init__(self, *args):\n",
        "#         super().__init__(*args)\n",
        "#         for layer in self:\n",
        "#             params = inspect.signature(layer.forward).parameters.keys()\n",
        "#             layer._fwdparams = ','.join(params)\n",
        "\n",
        "#     def forward(self, x, cond=None):\n",
        "#         for layer in self:\n",
        "#             args = [x]\n",
        "#             if 'cond' in layer._fwdparams: args.append(cond)\n",
        "#             x = layer(*args)\n",
        "#         return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "GjvZZswH1_KR"
      },
      "outputs": [],
      "source": [
        "# @title UpDownBlock_me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel=3, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        r = max(r, int(1/r))\n",
        "        out_ch = out_ch or in_ch\n",
        "        # if self.r>1: self.net = nn.Sequential(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        # elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        # # elif in_ch!=out_ch: self.net = ResBlock(in_ch, out_ch)\n",
        "        # else: self.net = ResBlock(in_ch, out_ch, emb_dim)\n",
        "        if self.r>1: self.net = Seq(ResBlock(in_ch, out_ch*r**2, emb_dim), nn.PixelShuffle(r))\n",
        "        elif self.r<1: self.net = Seq(nn.PixelUnshuffle(r), ResBlock(in_ch*r**2, out_ch, emb_dim))\n",
        "        else: self.net = Seq(ResBlock(in_ch, out_ch, emb_dim))\n",
        "\n",
        "    def forward(self, x, emb=None):\n",
        "        return self.net(x, emb)\n",
        "\n",
        "class UpDownBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel=7, r=1, emb_dim=None):\n",
        "        super().__init__()\n",
        "        self.block = PixelShuffleConv(in_ch, out_ch, kernel=kernel, r=r)\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w]\n",
        "        out = self.block(x, emb)\n",
        "        shortcut = F.adaptive_avg_pool3d(x, out.shape[1:]) # https://pytorch.org/docs/stable/nn.html#pooling-layers\n",
        "        return out + shortcut\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4kbIqclOK0t",
        "outputId": "549cc29b-84e0-449e-8ddb-0b4dee1dd4be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[32, 32, 32, 32]\n",
            "torch.Size([64, 3, 64, 64])\n",
            "931652\n"
          ]
        }
      ],
      "source": [
        "# @title U-DiT me\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x + self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        # self.rope = RoPE2D(d_head, h=64, w=64, base=100)\n",
        "        # print(d_head)\n",
        "        self.scale = d_head**-.5\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        bchw = x.shape\n",
        "        # b,c,h,w = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        # q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            # UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r)) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            # UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "            UpDownBlock(out_ch, out_ch, r=r) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, emb_dim=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, d_model), nn.SiLU(), nn.Linear(d_model, 3*emb_dim))\n",
        "\n",
        "        self.cond_emb = nn.Linear(cond_dim, 3*d_model)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 3\n",
        "        mult = [1,1,1,1] # [1,1,1,1] [1,2,3,4] [1,2,2,2]\n",
        "        # mult = [1,2,3,4] # [1,1,1,1] [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=1),\n",
        "        )\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "        self.skip_scale = nn.Parameter(torch.tensor(2**-.5))\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "        x = self.in_block(x)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**-.5], dim=1)\n",
        "            # print(len(self.up_list)-i)\n",
        "            # x = torch.cat([x, blocks[-i-1]*self.skip_scale**(len(self.up_list)-i)], dim=1) # https://arxiv.org/pdf/2310.13545\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "        return self.out(x)\n",
        "\n",
        "# norm,act,zeroconv < finallyr\n",
        "# attnblk =< uditblk(no down)?\n",
        "\n",
        "# nope, posemb input, rope qk\n",
        "\n",
        "# adaln, crossattn\n",
        "\n",
        "# downattn minute diff\n",
        "# no UpDownBlock\n",
        "\n",
        "# oom when noupdown\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "# model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "model = U_DiT(in_ch=3, d_model=32, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# uib<res\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0E7ovklT83O",
        "outputId": "a509a3d7-73be-4efa-f8fa-fe1179af9093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 3, 64, 64])\n",
            "116259\n"
          ]
        }
      ],
      "source": [
        "# @title U-DiT next\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x #+ self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class SelfAttn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        # self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.rope = RoPE2D(d_head, h=64, w=64, base=10000)\n",
        "        # print(d_head)\n",
        "        self.scale = d_head**-.5\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        bchw = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "\n",
        "        # q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        # q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "# class TimeEmb(nn.Module):\n",
        "#     def __init__(self, d_model, emb_dim):\n",
        "#         super().__init__()\n",
        "#         self.rot_emb = RotEmb(d_model)\n",
        "#         self.mlp = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "#     def forward(self, t): return self.mlp(self.rot_emb(t))\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class CombineAttnBlk(nn.Module):\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = SelfAttn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # if self.cond_dim!=None: self.cross = MultiHeadAttention(d_model, d_head=d_head, cond_dim=cond_dim, dropout=0)\n",
        "        self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), zero_module(nn.Linear(cond_dim, 6*d_model))) # adaptive layer norm zero (adaLN-Zero). very important!\n",
        "\n",
        "    def forward(self, x, cond=None, mask=None):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        if self.cond_dim!=None: x = x + gate_mca * self.cross((1 + scale_mca) * self.norm(x) + shift_mca, cond, mask)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, emb_dim=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        # self.time_emb = TimeEmb(d_model, 3*emb_dim)\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, d_model), nn.SiLU(), nn.Linear(d_model, 3*emb_dim))\n",
        "\n",
        "        self.cond_emb = nn.Linear(cond_dim, 3*d_model)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 1\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        # print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "        x = self.in_block(x)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "        return self.out(x)\n",
        "\n",
        "# norm,act,zeroconv < finallyr\n",
        "# attnblk =< uditblk(no down)?\n",
        "\n",
        "# nope, posemb input, rope qk\n",
        "\n",
        "# adaln, crossattn\n",
        "\n",
        "# downattn\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# uib<res\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "oFXiQfSn1i3u"
      },
      "outputs": [],
      "source": [
        "# @title unet to dit\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            # *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, emb_dim=None, cond_dim=16, depth=4, num_res_blocks=1, n_heads=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_heads = d_model // d_head\n",
        "\n",
        "        emb_dim = emb_dim or d_model# * 4\n",
        "        self.time_emb = nn.Sequential(RotEmb(d_model), nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "        # self.time_emb = TimeEmb(d_model, emb_dim)\n",
        "        self.cond_emb = nn.Linear(cond_dim, d_model)\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "        # self.pos_embed = nn.Parameter(torch.randn(1, self.extras + num_patches, embed_dim)*0.02)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            # U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=n_heads),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [b,c,h,w], [b], [b, cond_dim]\n",
        "        emb = self.time_emb(t) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "        cond = self.cond_emb(cond)\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "\n",
        "        # x = x + self.pos_embed\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print(\"unet fwd down\", x.shape)\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        # print(\"unet fwd mid1\", x.shape)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        # print(\"unet fwd mid2\", x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            # print(\"unet fwd up\", x.shape)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # 64,64 -vae-> 16,16 -unet->\n",
        "batch = 64\n",
        "cond_dim=10\n",
        "in_ch = 3\n",
        "model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=1).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # print(model)\n",
        "\n",
        "# x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1Nfr2mmxuzO",
        "outputId": "3d8447a9-3039-433e-b6bd-ef954df2ddbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "146512\n",
            "261712\n"
          ]
        }
      ],
      "source": [
        "# for name, param in model.named_parameters(): print(name, param.numel())\n",
        "print(sum(p.numel() for p in model.down_list.parameters() if p.requires_grad)) # 19683\n",
        "print(sum(p.numel() for p in model.up_list.parameters() if p.requires_grad)) # 19683\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "XwpnHW4wn9S1"
      },
      "outputs": [],
      "source": [
        "# @title data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_data = torchvision.datasets.MNIST(root=\"data\", train=True, download=True,transform=transforms.ToTensor(),) # do not normalise! want img in [0,1)\n",
        "test_data = torchvision.datasets.MNIST(root=\"data\", train=False, download=True, transform=transforms.ToTensor(),) #opt no download\n",
        "batch_size = 128 # 64 512\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Ubkj5kR6Or8V"
      },
      "outputs": [],
      "source": [
        "# @title gdown\n",
        "import pickle\n",
        "!gdown 1fYC7rJswDFpLeyywD56bu9ZjCQEyzRvY -O buffer512.pkl # S\n",
        "with open('buffer512.pkl', 'rb') as f: buffer = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "bHFHA_cXOvA5"
      },
      "outputs": [],
      "source": [
        "# @title buffer dataloader\n",
        "# RNNs https://colab.research.google.com/drive/16DZRFsBEPMTHnjDED1xlxBDZpCmp5XGR#scrollTo=IV5HmCFv_ITo\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "class BufferDataset(Dataset): # https://github.com/karpathy/minGPT\n",
        "    def __init__(self, buffer):\n",
        "        self.data = [step for episode in buffer for step in episode] # 0.00053\n",
        "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "        self.transform = transforms.Compose([transforms.ToTensor(),])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        state, action, reward = self.data[idx]\n",
        "        state = self.transform(state)\n",
        "        return state\n",
        "\n",
        "    def add(self, episode):\n",
        "        self.data.append(episode)\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "def imshow(img):\n",
        "    # img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(3, 3))\n",
        "    # print(npimg.shape) # (3, 64, 64)\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "train_data = BufferDataset(buffer) # one line of poem is roughly 50 characters\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "batch_size = 512 #512\n",
        "train_loader = DataLoader(train_data, shuffle = True, pin_memory = True, batch_size = batch_size, num_workers = 2, drop_last=True) # num_workers = 4\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf2bipgghY7O",
        "outputId": "1489f157-c51f-47bb-af2d-041e0b8fccca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/distributions/distribution.py:56: UserWarning: <class '__main__.LogitNormal'> does not define `arg_constraints`. Please set `arg_constraints = {}` or initialize the distribution with `validate_args=False` to turn off validation.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# @title LogitNormal\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LogitNormal(dist.Distribution):\n",
        "    def __init__(self, mu=0, std=.5):\n",
        "        super().__init__()\n",
        "        self.mu, self.std = mu, std\n",
        "        self._normal = dist.Normal(mu, std) # https://pytorch.org/docs/stable/distributions.html#normal\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        eps = self._normal.rsample(sample_shape)\n",
        "        return torch.sigmoid(eps) # https://en.wikipedia.org/wiki/Logit-normal_distribution\n",
        "\n",
        "logit_normal = LogitNormal()\n",
        "# samples = logit_normal.rsample((10,))\n",
        "# print(samples)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "QRPa3VWuQDIW"
      },
      "outputs": [],
      "source": [
        "# @title sampling timestep\n",
        "# def InverseSigmoid(x): return torch.log(x/(1-x))\n",
        "# def Normal(x, mu=0, std=.5): return torch.exp(-.5*((x-mu)/std)**2)/(std*(2*torch.pi)**2)\n",
        "# def LogitNormalPDF(x, mu=0, std=.5): return torch.nan_to_num(Normal(logit(x), mu, std) * 1/(x*(1-x)))\n",
        "\n",
        "# def invlogit(x): return torch.exp(x)/(1+torch.exp(x))\n",
        "# def InvLogitNormalCDF(x, mu=0, std=.5):\n",
        "#     cdf = invlogit(torch.erfinv(2*x-1)*(2**.5*std)+mu)\n",
        "#     cdf[x==1.] = 1 # lol, replace nan with 1 when x=1\n",
        "#     return cdf\n",
        "\n",
        "def logit(x): return torch.log(x/(1-x)) # x in (0,1)\n",
        "def LogitNormalCDF(x, mu=0, std=.5): # _/- for std<1.8; /-/ for std>1.8\n",
        "    cdf = 1/2 * (1 + torch.erf((logit(x)-mu)/(2**.5*std)))\n",
        "    return cdf\n",
        "\n",
        "def Cosine(x): return .5*(-torch.cos(torch.pi*x)+1) # _/-\n",
        "def Polynomial(x): return -2*x**3 + 3*x**2 # -2x^3 + 3x^2 # _/-\n",
        "def ACosine(x): return torch.acos(1-2*x)/torch.pi # /-/ # x = acos(1-2y)/pi\n",
        "def InvertCubic(x): return (x-1)**3+1 # /-\n",
        "def InvertExp(x, a=4): return (1-torch.exp(-a*x)) / (1-torch.exp(torch.tensor(-a))) # /-\n",
        "\n",
        "\n",
        "a, b = .0, 0\n",
        "def bezier(t, x0=0,y0=0, x1=a,y1=b, x2=1-a,y2=1-b, x3=1,y3=1):\n",
        "    # print(x1,y1)\n",
        "    # return ((1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3)), (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3)))\n",
        "    return (1-t)*((1-t)*((1-t)*x0+t*x1)+t*((1-t)*x1+t*x2))+t*((1-t)*((1-t)*x1+t*x2)+t*((1-t)*x2+t*x3))\n",
        "    # return (1-t)*((1-t)*((1-t)*y0+t*y1) +t*((1-t)*y1+t*y2))+t*((1-t)*((1-t)*y1+t*y2) +t*((1-t)*y2+t*y3))\n",
        "\n",
        "\n",
        "# x = torch.linspace(0, 1, 30)\n",
        "# y=x\n",
        "# y = LogitNormalCDF(x, mu=0, std=3) # _/- for std<1.8; /-/ for std>1.8\n",
        "# y = Cosine(x) # _/-\n",
        "# y = ACosine(x) # /-/\n",
        "# y = Polynomial(x) # _/-\n",
        "# y = InvertCubic(x) # /-\n",
        "# y = InvertExp(x) # /-\n",
        "# y = bezier(x) # _/-\n",
        "# print(x, y)\n",
        "# import matplotlib.pyplot as plt\n",
        "# plt.plot(x, y)\n",
        "# plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "OFc76OzlFnE1"
      },
      "outputs": [],
      "source": [
        "# @title Sampling\n",
        "import torchvision\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "def reverse_flow(unet, cond, timesteps=25): # [n_samples, cond_dim]\n",
        "    unet.eval()\n",
        "    i = torch.linspace(0, 1, timesteps+1)\n",
        "    # y = i # linear\n",
        "    # y = LogitNormalCDF(i, mu=0, std=3.) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    y = LogitNormalCDF(i, mu=-.0, std=2.7) # .5 _/- for std<1.8; 3 /-/ for std>1.8\n",
        "    # y = Cosine(i) # _/-\n",
        "    # y = ACosine(i) # /-/\n",
        "    # y = Polynomial(i) # _/-\n",
        "    # y = InvertCubic(i) # /-\n",
        "    # y = InvertExp(i) # /-\n",
        "    # y = bezier(i) # _/-\n",
        "\n",
        "    dt = y[1:]-y[:-1]\n",
        "    num_samples = cond.shape[0]\n",
        "    # x = torch.randn((num_samples, unet.in_ch, 16,16), device=device)\n",
        "    x = torch.randn((num_samples, 3, 64,64), device=device)\n",
        "    # cond = cond.repeat(num_samples,1) # [n_samples, cond_dim]\n",
        "    for y, dt in zip(y, dt):\n",
        "        # print(y, dt)\n",
        "        t = torch.full((num_samples,), y, device=device)  # Current time # [num_samples] 1. # torch.tensor(i * dt, device=device).repeat(n_samples)\n",
        "        with torch.no_grad():\n",
        "            model = lambda y,t: -unet(y, t, cond)\n",
        "            v = model(x, t)\n",
        "            x = x - dt * v # Euler update # 25steps:1sec\n",
        "\n",
        "            # k1 = model(x, t)\n",
        "            # k2 = model(x - 0.5 * dt * k1, t - 0.5 * dt)\n",
        "            # k3 = model(x - 0.5 * dt * k2, t - 0.5 * dt)\n",
        "            # k4 = model(x - dt * k3, t - dt)\n",
        "            # x = x - (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4) # RK4 update # 25steps:4.5sec\n",
        "    return x\n",
        "\n",
        "\n",
        "# # cond = F.one_hot(torch.tensor([3]*16, device=device), num_classes=10).to(torch.float)\n",
        "# cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "# # img_ = reverse_flow(model, cond, timesteps=10)\n",
        "# img_ = model.sample(cond)\n",
        "# # # plt.imshow(img_.cpu().squeeze())\n",
        "# # # plt.show()\n",
        "# imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbSq3Zx7aRL-"
      },
      "source": [
        "## main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "iJ1hbnSC12tA"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "def init_conv(conv, out_r=1, in_r=1):\n",
        "    o, i, h, w = conv.weight.shape\n",
        "    conv_weight = torch.empty(o//out_r**2, i//in_r**2, h, w)\n",
        "    nn.init.kaiming_uniform_(conv_weight)\n",
        "    conv.weight.data.copy_(conv_weight.repeat_interleave(out_r**2, dim=0).repeat_interleave(in_r**2, dim=1))\n",
        "    if conv.bias is not None: nn.init.zeros_(conv.bias)\n",
        "    return conv\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConv(in_ch, d_list[0], 7, r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], out_ch, 5, r=1/2), act,\n",
        "            # nn.Conv2d(in_ch, out_ch, 7, 2, 7//2), nn.MaxPool2d(2, 2),\n",
        "\n",
        "            # nn.PixelUnshuffle(4), init_conv(nn.Conv2d(3*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "            # nn.PixelUnshuffle(2), init_conv(nn.Conv2d(3*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelUnshuffle(2),\n",
        "            init_conv(nn.Conv2d(3*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelUnshuffle(4),\n",
        "        )\n",
        "            # nn.PixelUnshuffle(4), ResBlock(in_ch*4**2, out_ch, emb_dim=out_ch), AttentionBlock(out_ch),\n",
        "\n",
        "        # nn.init.zeros_(self.encoder.parameters()[-1])\n",
        "        # self.encoder[-2].apply(self.zero_conv_)\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConv(out_ch, d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConv(d_list[0], in_ch, 7, r=2),\n",
        "            # nn.Upsample(scale_factor=2), nn.ConvTranspose2d(out_ch, in_ch, 7, 2, 7//2, output_padding=1),# nn.BatchNorm2d(d_list[1]), nn.SiLU(),\n",
        "\n",
        "            # init_conv(nn.Conv2d(2*1**2, d_list[0]*4**2, 7, 1, padding=7//2), out_r=4, in_r=1), nn.PixelShuffle(4),\n",
        "            # nn.PixelShuffle(2), init_conv(nn.Conv2d(2*2**2, d_list[0]*2**2, 7, 1, padding=7//2), out_r=2, in_r=2), nn.PixelShuffle(2),\n",
        "            nn.PixelShuffle(4), init_conv(nn.Conv2d(2*4**2, d_list[0]*1**2, 7, 1, padding=7//2), out_r=1, in_r=4),\n",
        "        )\n",
        "            # ResBlock(out_ch, in_ch*4**2, emb_dim=in_ch*4**2), AttentionBlock(in_ch*4**2), nn.PixelShuffle(4),\n",
        "\n",
        "        # for param in self.parameters():\n",
        "        # nn.init.zeros_(self.decoder.parameters()[-1])\n",
        "    #     self.decoder[-1].apply(self.zero_conv_)\n",
        "\n",
        "    # def zero_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    #     if isinstance(conv, nn.Conv2d):\n",
        "    #         nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "in_ch=3\n",
        "# model_ch=16\n",
        "d_list=[16, 16] # [16,32]\n",
        "k_list=[7,5] # [7,5]\n",
        "# model = PixelAE(in_ch=in_ch, d_list=d_list, k_list=k_list)\n",
        "model = PixelAE(in_ch=in_ch, out_ch=16).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "input = torch.rand((4,in_ch,64,64), device=device)\n",
        "enc = model.encode(input)\n",
        "print(enc.shape)\n",
        "out = model.decode(enc)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Q43yKETuk3P"
      },
      "outputs": [],
      "source": [
        "model = PixelAE()\n",
        "# nn.init.zeros_(model.decoder.parameters()[-1])\n",
        "# print(model.decoder.parameters())\n",
        "# print(model.decoder[-2])\n",
        "# nn.init.zeros_(model.decoder[-2])\n",
        "# model.decoder[-2].apply(nn.init.zeros_)\n",
        "\n",
        "def init_conv_(conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "    if isinstance(conv, nn.Conv2d):\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "model.decoder[-2].apply(init_conv_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "nvBRca98P-GW"
      },
      "outputs": [],
      "source": [
        "# @title latent flow model\n",
        "\n",
        "class LFM(nn.Module): # latent flow model\n",
        "    def __init__(self, in_ch=3, d_list=[16, 3], d_model=16, cond_dim=16, depth=3, ae=None):\n",
        "        super().__init__()\n",
        "        self.ae = ae or PixelAE(in_ch, d_model)\n",
        "        self.unet = UNet(in_ch=in_ch, d_model=d_model, cond_dim=cond_dim, depth=3)\n",
        "\n",
        "    def loss(self, img, cond): # [b,c,h,w], [b,cond_dim]\n",
        "        x1 = self.ae.encode(img)\n",
        "        img_ = self.ae.decode(x1)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        fm_loss = otfm_loss(self.unet, x1.detach(), cond)\n",
        "        return ae_loss, fm_loss\n",
        "        # loss = ae_loss + fm_loss\n",
        "        # return loss\n",
        "\n",
        "    def ae_loss(self, img):\n",
        "        img_ = self.ae(img)\n",
        "        ae_loss = F.mse_loss(img_, img)\n",
        "        return ae_loss\n",
        "\n",
        "\n",
        "    # self.unet(x, t=None, cond=None)\n",
        "    # def forward(self, cond, timesteps=10): # [N, C, ...]\n",
        "    def sample(self, cond=None, timesteps=10):\n",
        "        self.eval()\n",
        "        # cond = F.one_hot(torch.tensor([4]*16, device=device), num_classes=10).to(torch.float)\n",
        "        # if cond is None: cond = F.one_hot(torch.arange(16, dtype=torch.float, device=device)%10, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x1_ = reverse_flow(self.unet, cond, timesteps=timesteps)\n",
        "            img_ = self.ae.decode(x1_)\n",
        "        return img_\n",
        "\n",
        "model = LFM(d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "# model = LFM(ae=model.ae, d_list=[16, 16], d_model=16, cond_dim=10, depth=3).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=3e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "rfsabyM8P7q3",
        "outputId": "a1edc119-01e9-43e2-8abd-2eb76d8acfbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 1.111527919769287\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.6293409..2.609356].\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 1.042733907699585\n",
            "20 1.0143589973449707\n",
            "30 0.9391114711761475\n",
            "40 0.8012310266494751\n",
            "50 0.6767311096191406\n",
            "60 0.5447220206260681\n",
            "70 0.4110869765281677\n",
            "80 0.3084096610546112\n",
            "90 0.21572625637054443\n",
            "100 0.15216538310050964\n",
            "110 0.1198841854929924\n",
            "120 0.09404214471578598\n",
            "130 0.08209019899368286\n"
          ]
        }
      ],
      "source": [
        "# @title train\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "scaler = torch.GradScaler(device)\n",
        "\n",
        "def otfm_loss(model, x1, cond, sig_min = 0.001, eps = 1e-5): # UNetModel, [b,c,h,w], [b,cond_dim] # https://github.com/lebellig/flow-matching/blob/main/Flow_Matching.ipynb\n",
        "    batch = x1.size(0)\n",
        "    # t = torch.rand((batch,), device=device) % (1 - eps)\n",
        "    t = logit_normal.rsample((batch,)).to(device) # in [0,1] [batch,1]\n",
        "    t_ = t[...,None,None,None]\n",
        "    x0 = torch.randn_like(x1)\n",
        "    psi_t = (1 - (1-sig_min)*t_)*x0 + t_*x1 # t(x) = (1  (1  min)t)x + tx1, (22)\n",
        "    v_psi = model(psi_t, t, cond) # vt(t(x0))\n",
        "    d_psi = x1 - (1 - sig_min) * x0 #\n",
        "    return F.mse_loss(v_psi, d_psi) # LCFM()\n",
        "\n",
        "\n",
        "def ae_train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "            # loss = otfm_loss(model, x1, cond) # unet\n",
        "            # img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # loss = model.ae_loss(img)\n",
        "\n",
        "            x1 = model.ae.encode(img)\n",
        "            img_ = model.ae.decode(x1)\n",
        "            ae_loss = F.mse_loss(img_, img)\n",
        "            loss = ae_loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "            imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "            imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"ae_loss\": loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "\n",
        "def train(model, optim, dataloader):\n",
        "    model.train()\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "    # for i, img in enumerate(dataloader):\n",
        "    #     img = img.to(device)\n",
        "        # # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "        # x1 = F.interpolate(x1, size=(16,16)).repeat(1,3,1,1)\n",
        "        img = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "\n",
        "            # with torch.no_grad():\n",
        "            # x1 = model.ae.encode(img)\n",
        "            # img_ = model.ae.decode(x1)\n",
        "            # loss = F.mse_loss(img_, img)\n",
        "\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # with torch.no_grad():\n",
        "            #     x1 = model.ae.encode(img)\n",
        "            #     img_ = model.ae.decode(x1)\n",
        "            # fm_loss = otfm_loss(model.unet, x1.detach(), cond)\n",
        "            loss = otfm_loss(model, img, cond)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # clip gradients\n",
        "        scaler.step(optim)\n",
        "        scaler.update()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(i,loss.item())\n",
        "            # print(i,ae_loss.item(), fm_loss.item())\n",
        "        if i % 200 == 0:\n",
        "            with torch.no_grad():\n",
        "                cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "                img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "                # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "                # x1_ = reverse_flow(model.unet, cond, timesteps=10)\n",
        "                # print(x1.dtype)\n",
        "                # imshow(torchvision.utils.make_grid(x1_[:4].cpu(), nrow=4))\n",
        "\n",
        "                # img_ = model.ae.decode(x1_.float())\n",
        "                # with torch.no_grad(): img_ = model.ae.decode(x1_)\n",
        "                imshow(torchvision.utils.make_grid(img_[:4].cpu(), nrow=4))\n",
        "\n",
        "        try: wandb.log({\"loss\": loss.item()})\n",
        "        # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "        except NameError: pass\n",
        "\n",
        "for epoch in range(1):\n",
        "# for epoch in range(40):\n",
        "    train(model, optim, train_loader)\n",
        "\n",
        "    # for x,y in test_loader: break\n",
        "    # img = F.interpolate(x.to(device), size=(64,64)).repeat(1,3,1,1)\n",
        "    # x1 = model.ae.encode(img)\n",
        "    # img_ = model.ae.decode(x1)\n",
        "    # imshow(torchvision.utils.make_grid(img[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(x1[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "    # imshow(torchvision.utils.make_grid(img_[:4].detach().cpu().to(torch.float32), nrow=4))\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # # # cond = F.one_hot(torch.tensor([4], device=device), num_classes=10).expand(16,-1).to(torch.float)\n",
        "        cond = F.one_hot(torch.arange(16, device=device)%10, num_classes=10).to(torch.float)\n",
        "        img_ = reverse_flow(model, cond, timesteps=10) # unet\n",
        "        # img_ = model.sample(cond, timesteps=10) # ldm\n",
        "        imshow(torchvision.utils.make_grid(img_.cpu(), nrow=4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ecSX2oJJ4_2F"
      },
      "outputs": [],
      "source": [
        "# @title test func\n",
        "\n",
        "def test(model, dataloader):\n",
        "    model.eval()\n",
        "    total_loss=0\n",
        "    for i, (x1, y) in enumerate(dataloader):\n",
        "        x1, y = x1.to(device), y.to(device)\n",
        "        # cond = cond_emb(y)\n",
        "        cond = F.one_hot(y, num_classes=10).to(torch.float)\n",
        "\n",
        "        with torch.autocast(device_type=device, dtype=torch.bfloat16): # float16 cannot?\n",
        "            # x1 = F.interpolate(x1, size=(64,64)).repeat(1,3,1,1)\n",
        "            # ae_loss, fm_loss = model.loss(x1, cond)\n",
        "            # loss = ae_loss + fm_loss\n",
        "            x1 = F.interpolate(x1, size=(16,16))#.repeat(1,3,1,1)\n",
        "            loss = otfm_loss(model, x1, cond) # unet\n",
        "        total_loss+=loss\n",
        "    print(total_loss/len(dataloader))\n",
        "    # if i % 10 == 0: print(i,ae_loss.item(),fm_loss.item())\n",
        "    try: wandb.log({\"test_loss\": loss.item()})\n",
        "    # try: wandb.log({\"ae_loss\": ae_loss.item(), \"fm_loss\": fm_loss.item()})\n",
        "    except: pass\n",
        "\n",
        "\n",
        "# test(model, test_loader)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "2Nd-sGe6Ku4S",
        "outputId": "725ee461-cc88-48ba-c76b-27d2f60aa27d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.38856</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">different-bee-84</strong> at: <a href='https://wandb.ai/bobdole/lfm/runs/btl772dw' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/btl772dw</a><br> View project at: <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250328_014236-btl772dw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250328_015234-svnmocbj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/bobdole/lfm/runs/svnmocbj' target=\"_blank\">hearty-capybara-85</a></strong> to <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/bobdole/lfm' target=\"_blank\">https://wandb.ai/bobdole/lfm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/bobdole/lfm/runs/svnmocbj' target=\"_blank\">https://wandb.ai/bobdole/lfm/runs/svnmocbj</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title wandb\n",
        "!pip install -q wandb\n",
        "import wandb # https://docs.wandb.ai/quickstart\n",
        "wandb.login(key='487a2109e55dce4e13fc70681781de9f50f27be7')\n",
        "try: run.finish()\n",
        "except NameError: pass\n",
        "run = wandb.init(project=\"lfm\", config={\"model\": \"res18\",})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jGN0e0Mxe5UI"
      },
      "outputs": [],
      "source": [
        "# @title save/load\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder='/content/drive/MyDrive/jepa/'\n",
        "\n",
        "# modelsd, optimsd = torch.load(folder+'lfm.pkl', map_location=device).values()\n",
        "# model.load_state_dict(modelsd, strict=False)\n",
        "# optim.load_state_dict(optimsd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrFxTtw4eFSq"
      },
      "outputs": [],
      "source": [
        "checkpoint = {'model': model.state_dict(), 'optimizer': optim.state_dict()}\n",
        "torch.save(checkpoint, folder+'lfm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVSdLd7um-Rs"
      },
      "outputs": [],
      "source": [
        "# @title WCDM\n",
        "# High Fidelity Visualization of What Your Self-Supervised Representation Knows About aug 2022\n",
        "# https://arxiv.org/pdf/2112.09164\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFhmfVnrOiQd"
      },
      "outputs": [],
      "source": [
        "# # dataiter = iter(train_data)\n",
        "# x,y = next(dataiter)\n",
        "# # print(x)\n",
        "# print(x.shape)\n",
        "# x=x.unsqueeze(0)\n",
        "# # x1 = F.interpolate(x, size=(16,16))#.repeat(1,3,1,1)\n",
        "# x1 = F.interpolate(x, size=(64,64)).repeat(1,3,1,1)\n",
        "# imshow(torchvision.utils.make_grid(x1.cpu(), nrow=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQlJ4XVLrOSx"
      },
      "source": [
        "## save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r4htE9Mz-JiO"
      },
      "outputs": [],
      "source": [
        "# @title U-DiT next\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "class LayerNorm2d(nn.RMSNorm): # LayerNorm RMSNorm\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "    def forward(self, x): return super().forward(x.permute(0, 2, 3, 1)).permute(0, 3, 1, 2)\n",
        "\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = x #+ self.layer(x)\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "# conv res pixeldown\n",
        "\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, n_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim, self.heads, self.r = dim, n_heads, r\n",
        "        d_head = dim//n_heads\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(d_head, seq_len=512, base=10000)\n",
        "        self.scale = d_head**-.5 # v1\n",
        "\n",
        "        # self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        bchw = x.shape\n",
        "        # x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        x = x.flatten(2).transpose(-2,-1)\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        x = x.transpose(-2,-1).reshape(bchw) # [batch, n_heads, T/num_tok, d_head] -> [batch, n_heads*d_head, T/num_tok] -> [b,c,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "\n",
        "class TimeEmb(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(nn.Linear(emb_dim, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "    def forward(self, t): return self.mlp(self.rot_emb(t))\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, d_model, cond_dim, n_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.attn = DownSample_Attn(d_model, n_heads=n_heads, r=down_factor)\n",
        "        # self.mlp = FeedForward(d_model)\n",
        "        self.mlp = UIB(d_model, mult=4)\n",
        "        # self.mlp = ResBlock(d_model)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(cond_dim, 6 * d_model))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # print('U_DiT blk', x.shape, self.d_model, cond.shape)\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1) # [batch, d_model, 1, 1]\n",
        "        # print('U_DiT blk', x.shape, self.d_model, shift_mlp.shape)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "        return x\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, d_model, out_ch):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(d_model, d_model, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(d_model, 2*d_model))\n",
        "        )\n",
        "        self.norm = LayerNorm2d(d_model, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm = LayerNorm2d(d_model)\n",
        "        self.out_proj = zero_module(nn.Conv2d(d_model, out_ch, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        shift, scale = self.adaLN_modulation(cond)[...,None,None].chunk(2, dim=1)\n",
        "        # print('FinalLayer', x.shape, shift.shape, scale.shape)\n",
        "        x = self.in_proj(x)\n",
        "        x = (1 + scale) * self.norm(x) + shift\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim=None, cond_dim=None, n_heads=None, d_head=8, depth=1, r=1):\n",
        "        super().__init__()\n",
        "        n_heads = n_heads or out_ch//d_head\n",
        "        self.seq = Seq(\n",
        "            UpDownBlock(in_ch, out_ch, r=min(1,r), emb_dim=emb_dim) if in_ch != out_ch or r<1 else nn.Identity(),\n",
        "            # AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            *[U_DiTBlock(out_ch, cond_dim, n_heads) for i in range(1)],\n",
        "            UpDownBlock(out_ch, out_ch, r=r, emb_dim=emb_dim) if r>1 else nn.Identity(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_ch=3, d_model=96, out_ch=None, cond_dim=16, depth=[2,5,8,5,2], n_heads=16, d_head=4):\n",
        "        super().__init__()\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.time_emb = TimeEmb(d_model*3)\n",
        "        self.cond_emb = nn.Linear(cond_dim, d_model*3)\n",
        "        self.in_block = nn.Conv2d(in_ch, d_model, 3, 1, 3//2)\n",
        "\n",
        "        depth = 3\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "        # print(ch_list)\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 1/2) for i in range(depth-1)])\n",
        "        emb_dim=None\n",
        "        self.middle_block = Seq(\n",
        "            UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth-1], ch_list[depth], r=1/2),\n",
        "            # AttentionBlock(ch_list[depth], d_head, cond_dim=d_model),\n",
        "            U_DiTBlock(ch_list[depth], cond_dim=d_model, n_heads=d_model//d_head),\n",
        "            UpDownBlock(ch_list[depth], ch_list[depth-1], r=2, emb_dim=emb_dim),\n",
        "            # UpDownBlock(ch_list[depth], ch_list[depth-1], r=2),\n",
        "        )\n",
        "\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], cond_dim=d_model, n_heads=n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, n_heads, depth=1, r=1 if i==0 else 2) for i in reversed(range(depth-1))])\n",
        "\n",
        "        # self.out = FinalLayer(d_model, in_ch) # udit\n",
        "        # # self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # # self.out = nn.Conv2d(d_model, out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.in_block(x) # (N, C, H, W)\n",
        "        c123 = self.time_emb(t) + self.cond_emb(y)\n",
        "        cond = c123.chunk(3, dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            # print('U_DiT down', x.shape)\n",
        "            x = down(x, cond=cond[i])\n",
        "            blocks.append(x)\n",
        "        # print('U_DiT mid1', x.shape)\n",
        "        x = self.middle_block(x, cond=cond[-1])\n",
        "        # print('U_DiT mid2', x.shape)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print('U_DiT up', x.shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond=cond[-1-i])\n",
        "\n",
        "        # x = self.final_layer(x, cond=cond[0]) # (N, T, patch_size ** 2 * out_ch)\n",
        "        return self.out(x)\n",
        "        # return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs): return U_DiT(down_factor=2, d_model=96, n_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(d_model=192, n_heads=8,\n",
        "# def U_DiT_L: U_DiT(d_model=384, n_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_ch=3, d_model=16, n_heads=4, depth=[1], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "out = model(inputs, t, y)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# uib<res\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "B4R77KQvNgj4"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        # self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "        self.res_conv = zero_module(nn.Conv2d(in_ch, out_ch, 1)) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head=4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        mult = [1,2,3,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in mult[:depth+1]] # [128, 256, 384, 512]\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        # self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), zero_module(nn.Conv2d(d_model, out_ch, 3, padding=1))) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x) if self.in_ch!=self.d_model else x\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x) #if self.out_ch!=self.d_model else x\n",
        "\n",
        "\n",
        "\n",
        "# # # 64,64 -vae-> 16,16 -unet->\n",
        "batch = 64\n",
        "cond_dim=10\n",
        "in_ch = 3\n",
        "model = UNet(in_ch=in_ch, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 5311187\n",
        "# # # print(model)\n",
        "\n",
        "# # x=torch.rand((batch,in_ch,16,16),device=device)\n",
        "# x=torch.rand((batch,in_ch,64,64),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "# optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# # # cond_emb = nn.Embedding(10, cond_dim).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "kJOnXmd2jGga"
      },
      "outputs": [],
      "source": [
        "# @title facebookresearch/DiT\n",
        "# https://github.com/facebookresearch/DiT/blob/main/models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from timm.models.vision_transformer import PatchEmbed, Attention, Mlp\n",
        "# https://github.com/huggingface/pytorch-image-models/blob/main/timm/models/vision_transformer.py#L58\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, frequency_embedding_size=256):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(frequency_embedding_size, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        self.frequency_embedding_size = frequency_embedding_size\n",
        "\n",
        "    @staticmethod\n",
        "    def timestep_embedding(t, dim, max_period=10000):\n",
        "        \"\"\"\n",
        "        Create sinusoidal timestep embeddings.\n",
        "        :param t: a 1-D Tensor of N indices, one per batch element.\n",
        "                          These may be fractional.\n",
        "        :param dim: the dimension of the output.\n",
        "        :param max_period: controls the minimum frequency of the embeddings.\n",
        "        :return: an (N, D) Tensor of positional embeddings.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/glide_text2im/nn.py\n",
        "        half = dim // 2\n",
        "        freqs = torch.exp(-math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half).to(device=t.device)\n",
        "        args = t[:, None].float() * freqs[None]\n",
        "        embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "        if dim % 2:\n",
        "            embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "        return embedding\n",
        "\n",
        "    def forward(self, t):\n",
        "        t_freq = self.timestep_embedding(t, self.frequency_embedding_size)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class LabelEmbedder(nn.Module):\n",
        "    \"\"\"\n",
        "    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, hidden_size, dropout_prob):\n",
        "        super().__init__()\n",
        "        use_cfg_embedding = dropout_prob > 0\n",
        "        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)\n",
        "        self.num_classes = num_classes\n",
        "        self.dropout_prob = dropout_prob\n",
        "\n",
        "    def token_drop(self, labels, force_drop_ids=None):\n",
        "        \"\"\"\n",
        "        Drops labels to enable classifier-free guidance.\n",
        "        \"\"\"\n",
        "        if force_drop_ids is None:\n",
        "            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob\n",
        "        else:\n",
        "            drop_ids = force_drop_ids == 1\n",
        "        labels = torch.where(drop_ids, self.num_classes, labels)\n",
        "        return labels\n",
        "\n",
        "    def forward(self, labels, train, force_drop_ids=None):\n",
        "        use_dropout = self.dropout_prob > 0\n",
        "        if (train and use_dropout) or (force_drop_ids is not None):\n",
        "            labels = self.token_drop(labels, force_drop_ids)\n",
        "        embeddings = self.embedding_table(labels)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale.unsqueeze(1)) + shift.unsqueeze(1)\n",
        "\n",
        "class DiTBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A DiT block with adaptive layer norm zero (adaLN-Zero) conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, mlp_ratio=4.0, **block_kwargs):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.attn = Attention(hidden_size, num_heads=num_heads, qkv_bias=True, **block_kwargs)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        mlp_hidden_dim = int(hidden_size * mlp_ratio)\n",
        "        approx_gelu = lambda: nn.GELU(approximate=\"tanh\")\n",
        "        self.mlp = Mlp(in_features=hidden_size, hidden_features=mlp_hidden_dim, act_layer=approx_gelu, drop=0)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 6 * hidden_size, bias=True))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(c).chunk(6, dim=1)\n",
        "        x = x + gate_msa.unsqueeze(1) * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        x = x + gate_mlp.unsqueeze(1) * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "        return x\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, patch_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.norm_final = nn.LayerNorm(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        self.linear = nn.Linear(hidden_size, patch_size * patch_size * out_channels)\n",
        "        self.adaLN_modulation = nn.Sequential(nn.SiLU(), nn.Linear(hidden_size, 2 * hidden_size))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.linear(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DiT(nn.Module):\n",
        "    def __init__(self, input_size=32, patch_size=2, in_channels=4, hidden_size=1152, depth=28,\n",
        "        num_heads=16, mlp_ratio=4.0, class_dropout_prob=0.1, num_classes=1000, learn_sigma=True):\n",
        "        super().__init__()\n",
        "        # self.learn_sigma = learn_sigma\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels * 2 if learn_sigma else in_channels\n",
        "        # self.patch_size = patch_size\n",
        "        # self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = PatchEmbed(input_size, patch_size, in_channels, hidden_size, bias=True)\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size)\n",
        "        self.y_embedder = LabelEmbedder(num_classes, hidden_size, class_dropout_prob)\n",
        "        num_patches = self.x_embedder.num_patches\n",
        "        # Will use fixed sin-cos embedding:\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches, hidden_size), requires_grad=False)\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            DiTBlock(hidden_size, num_heads, mlp_ratio=mlp_ratio) for _ in range(depth)\n",
        "        ])\n",
        "        self.final_layer = FinalLayer(hidden_size, patch_size, self.out_channels)\n",
        "        self.initialize_weights()\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        # Initialize transformer layers:\n",
        "        def _basic_init(module):\n",
        "            if isinstance(module, nn.Linear):\n",
        "                torch.nn.init.xavier_uniform_(module.weight)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.constant_(module.bias, 0)\n",
        "        self.apply(_basic_init)\n",
        "\n",
        "        # Initialize (and freeze) pos_embed by sin-cos embedding:\n",
        "        pos_embed = get_2d_sincos_pos_embed(self.pos_embed.shape[-1], int(self.x_embedder.num_patches ** 0.5))\n",
        "        self.pos_embed.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))\n",
        "\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.proj.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.proj.bias, 0)\n",
        "\n",
        "        # Initialize label embedding table:\n",
        "        nn.init.normal_(self.y_embedder.embedding_table.weight, std=0.02)\n",
        "\n",
        "        # Initialize timestep embedding MLP:\n",
        "        nn.init.normal_(self.t_embedder.mlp[0].weight, std=0.02)\n",
        "        nn.init.normal_(self.t_embedder.mlp[2].weight, std=0.02)\n",
        "\n",
        "        # Zero-out adaLN modulation layers in DiT blocks:\n",
        "        for block in self.blocks:\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].weight, 0)\n",
        "            nn.init.constant_(block.adaLN_modulation[-1].bias, 0)\n",
        "\n",
        "        # Zero-out output layers:\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].weight, 0)\n",
        "        nn.init.constant_(self.final_layer.adaLN_modulation[-1].bias, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.weight, 0)\n",
        "        nn.init.constant_(self.final_layer.linear.bias, 0)\n",
        "\n",
        "    def unpatchify(self, x):\n",
        "        \"\"\"\n",
        "        x: (N, T, patch_size**2 * C)\n",
        "        imgs: (N, H, W, C)\n",
        "        \"\"\"\n",
        "        c = self.out_channels\n",
        "        p = self.x_embedder.patch_size[0]\n",
        "        h = w = int(x.shape[1] ** 0.5)\n",
        "        assert h * w == x.shape[1]\n",
        "\n",
        "        x = x.reshape(shape=(x.shape[0], h, w, p, p, c))\n",
        "        x = torch.einsum('nhwpqc->nchpwq', x)\n",
        "        imgs = x.reshape(shape=(x.shape[0], c, h * p, h * p))\n",
        "        return imgs\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], [b], [b]\n",
        "        x = self.x_embedder(x) + self.pos_embed  # (N, T, D), where T = H * W / patch_size ** 2\n",
        "        t = self.t_embedder(t)                   # (N, D)\n",
        "        y = self.y_embedder(y, self.training)    # (N, D)\n",
        "        c = t + y                                # (N, D)\n",
        "        for block in self.blocks:\n",
        "            x = block(x, c)                      # (N, T, D)\n",
        "        x = self.final_layer(x, c)                # (N, T, patch_size ** 2 * out_channels)\n",
        "        x = self.unpatchify(x)                   # (N, out_channels, H, W)\n",
        "        return x\n",
        "\n",
        "    def forward_with_cfg(self, x, t, y, cfg_scale):\n",
        "        \"\"\"\n",
        "        Forward pass of DiT, but also batches the unconditional forward pass for classifier-free guidance.\n",
        "        \"\"\"\n",
        "        # https://github.com/openai/glide-text2im/blob/main/notebooks/text2im.ipynb\n",
        "        half = x[: len(x) // 2]\n",
        "        combined = torch.cat([half, half], dim=0)\n",
        "        model_out = self.forward(combined, t, y)\n",
        "        # For exact reproducibility reasons, we apply classifier-free guidance on only\n",
        "        # three channels by default. The standard approach to cfg applies it to all channels.\n",
        "        # This can be done by uncommenting the following line and commenting-out the line following that.\n",
        "        # eps, rest = model_out[:, :self.in_channels], model_out[:, self.in_channels:]\n",
        "        eps, rest = model_out[:, :3], model_out[:, 3:]\n",
        "        cond_eps, uncond_eps = torch.split(eps, len(eps) // 2, dim=0)\n",
        "        half_eps = uncond_eps + cfg_scale * (cond_eps - uncond_eps)\n",
        "        eps = torch.cat([half_eps, half_eps], dim=0)\n",
        "        return torch.cat([eps, rest], dim=1)\n",
        "\n",
        "\n",
        "# def DiT_XL_2/4/8(**kwargs): return DiT(depth=28, hidden_size=1152, patch_size=2/4/8, num_heads=16, **kwargs)\n",
        "# def DiT_L_2(**kwargs): return DiT(depth=24, hidden_size=1024, patch_size=2, num_heads=16, **kwargs)\n",
        "# def DiT_B_2(**kwargs): return DiT(depth=12, hidden_size=768, patch_size=2, num_heads=12, **kwargs)\n",
        "# def DiT_S_2(**kwargs): return DiT(depth=12, hidden_size=384, patch_size=2, num_heads=6, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IWmZcwoEft7y"
      },
      "outputs": [],
      "source": [
        "# @title baofff/U-ViT\n",
        "# https://github.com/baofff/U-ViT/blob/main/libs/uvit.py#L138\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from .timm import trunc_normal_, Mlp\n",
        "import einops\n",
        "import torch.utils.checkpoint\n",
        "\n",
        "if hasattr(torch.nn.functional, 'scaled_dot_product_attention'):\n",
        "    ATTENTION_MODE = 'flash'\n",
        "else:\n",
        "    try:\n",
        "        import xformers\n",
        "        import xformers.ops\n",
        "        ATTENTION_MODE = 'xformers'\n",
        "    except:\n",
        "        ATTENTION_MODE = 'math'\n",
        "print(f'attention mode is {ATTENTION_MODE}')\n",
        "\n",
        "\n",
        "def timestep_embedding(timesteps, dim, max_period=10000):\n",
        "    \"\"\"\n",
        "    Create sinusoidal timestep embeddings.\n",
        "\n",
        "    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n",
        "                      These may be fractional.\n",
        "    :param dim: the dimension of the output.\n",
        "    :param max_period: controls the minimum frequency of the embeddings.\n",
        "    :return: an [N x dim] Tensor of positional embeddings.\n",
        "    \"\"\"\n",
        "    half = dim // 2\n",
        "    freqs = torch.exp(\n",
        "        -math.log(max_period) * torch.arange(start=0, end=half, dtype=torch.float32) / half\n",
        "    ).to(device=timesteps.device)\n",
        "    args = timesteps[:, None].float() * freqs[None]\n",
        "    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n",
        "    if dim % 2:\n",
        "        embedding = torch.cat([embedding, torch.zeros_like(embedding[:, :1])], dim=-1)\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def patchify(imgs, patch_size):\n",
        "    x = einops.rearrange(imgs, 'B C (h p1) (w p2) -> B (h w) (p1 p2 C)', p1=patch_size, p2=patch_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "def unpatchify(x, channels=3):\n",
        "    patch_size = int((x.shape[2] // channels) ** 0.5)\n",
        "    h = w = int(x.shape[1] ** .5)\n",
        "    assert h * w == x.shape[1] and patch_size ** 2 * channels == x.shape[2]\n",
        "    x = einops.rearrange(x, 'B (h w) (p1 p2 C) -> B C (h p1) (w p2)', h=h, p1=patch_size, p2=patch_size)\n",
        "    return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = qk_scale or head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L, C = x.shape\n",
        "\n",
        "        qkv = self.qkv(x)\n",
        "        if ATTENTION_MODE == 'flash':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads).float()\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
        "            x = torch.nn.functional.scaled_dot_product_attention(q, k, v)\n",
        "            x = einops.rearrange(x, 'B H L D -> B L (H D)')\n",
        "        elif ATTENTION_MODE == 'xformers':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B L H D', K=3, H=self.num_heads)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B L H D\n",
        "            x = xformers.ops.memory_efficient_attention(q, k, v)\n",
        "            x = einops.rearrange(x, 'B L H D -> B L (H D)', H=self.num_heads)\n",
        "        elif ATTENTION_MODE == 'math':\n",
        "            qkv = einops.rearrange(qkv, 'B L (K H D) -> K B H L D', K=3, H=self.num_heads)\n",
        "            q, k, v = qkv[0], qkv[1], qkv[2]  # B H L D\n",
        "            attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "            attn = attn.softmax(dim=-1)\n",
        "            attn = self.attn_drop(attn)\n",
        "            x = (attn @ v).transpose(1, 2).reshape(B, L, C)\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None,\n",
        "                 act_layer=nn.GELU, norm_layer=nn.LayerNorm, skip=False, use_checkpoint=False):\n",
        "        super().__init__()\n",
        "        self.norm1 = norm_layer(dim)\n",
        "        self.attn = Attention(\n",
        "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer)\n",
        "        self.skip_linear = nn.Linear(2 * dim, dim) if skip else None\n",
        "        self.use_checkpoint = use_checkpoint\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        if self.use_checkpoint:\n",
        "            return torch.utils.checkpoint.checkpoint(self._forward, x, skip)\n",
        "        else:\n",
        "            return self._forward(x, skip)\n",
        "\n",
        "    def _forward(self, x, skip=None):\n",
        "        if self.skip_linear is not None:\n",
        "            x = self.skip_linear(torch.cat([x, skip], dim=-1))\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    \"\"\" Image to Patch Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self, patch_size, in_chans=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        assert H % self.patch_size == 0 and W % self.patch_size == 0\n",
        "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UViT(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768, depth=12, num_heads=12, mlp_ratio=4.,\n",
        "                 qkv_bias=False, qk_scale=None, norm_layer=nn.LayerNorm, mlp_time_embed=False, num_classes=-1,\n",
        "                 use_checkpoint=False, conv=True, skip=True):\n",
        "        super().__init__()\n",
        "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
        "        self.num_classes = num_classes\n",
        "        self.in_chans = in_chans\n",
        "\n",
        "        self.patch_embed = PatchEmbed(patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
        "        num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim),\n",
        "        ) if mlp_time_embed else nn.Identity()\n",
        "\n",
        "        if self.num_classes > 0:\n",
        "            self.label_emb = nn.Embedding(self.num_classes, embed_dim)\n",
        "            self.extras = 2\n",
        "        else:\n",
        "            self.extras = 1\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.extras + num_patches, embed_dim))\n",
        "\n",
        "        self.in_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
        "            for _ in range(depth // 2)])\n",
        "\n",
        "        self.mid_block = Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, use_checkpoint=use_checkpoint)\n",
        "\n",
        "        self.out_blocks = nn.ModuleList([\n",
        "            Block(\n",
        "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
        "                norm_layer=norm_layer, skip=skip, use_checkpoint=use_checkpoint)\n",
        "            for _ in range(depth // 2)])\n",
        "\n",
        "        self.norm = norm_layer(embed_dim)\n",
        "        self.patch_dim = patch_size ** 2 * in_chans\n",
        "        self.decoder_pred = nn.Linear(embed_dim, self.patch_dim, bias=True)\n",
        "        self.final_layer = nn.Conv2d(self.in_chans, self.in_chans, 3, padding=1) if conv else nn.Identity()\n",
        "\n",
        "        trunc_normal_(self.pos_embed, std=.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {'pos_embed'}\n",
        "\n",
        "    def forward(self, x, timesteps, y=None):\n",
        "        x = self.patch_embed(x)\n",
        "        B, L, D = x.shape\n",
        "\n",
        "        time_token = self.time_embed(timestep_embedding(timesteps, self.embed_dim))\n",
        "        time_token = time_token.unsqueeze(dim=1)\n",
        "        x = torch.cat((time_token, x), dim=1)\n",
        "        if y is not None:\n",
        "            label_emb = self.label_emb(y)\n",
        "            label_emb = label_emb.unsqueeze(dim=1)\n",
        "            x = torch.cat((label_emb, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        skips = []\n",
        "        for blk in self.in_blocks:\n",
        "            x = blk(x)\n",
        "            skips.append(x)\n",
        "\n",
        "        x = self.mid_block(x)\n",
        "\n",
        "        for blk in self.out_blocks:\n",
        "            x = blk(x, skips.pop())\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = self.decoder_pred(x)\n",
        "        assert x.size(1) == self.extras + L\n",
        "        x = x[:, self.extras:, :]\n",
        "        x = unpatchify(x, self.in_chans)\n",
        "        x = self.final_layer(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "G5l1b6dUu6MJ"
      },
      "outputs": [],
      "source": [
        "# @title U-DiT\n",
        "# https://github.com/YuchuanTian/U-DiT/blob/main/udit_models.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def zero_module(module):\n",
        "    for p in module.parameters():\n",
        "        p.detach().zero_()\n",
        "    return module\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "# class LayerNorm2d(nn.LayerNorm):\n",
        "class LayerNorm2d(nn.RMSNorm):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 3, 1)\n",
        "        x = super().forward(x)\n",
        "        x = x.permute(0, 3, 1, 2)\n",
        "        return x\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "class DownSampler(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=5, r=2):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.layer = nn.Conv2d(dim, dim, kernel_size, 1, kernel_size//2, groups=dim)\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "        x = self.layer(x) + x # down_shortcut\n",
        "        return F.pixel_unshuffle(x.transpose(0,1), self.r).flatten(2).permute(1,2,0) # [b,c,h*r,w*r] -> [c,b*r^2,h,w] -> [b*r^2,h*w,c]\n",
        "\n",
        "# conv res pixeldown\n",
        "\n",
        "class DownSample_Attn(nn.Module):\n",
        "    def __init__(self, dim, num_heads, r=2):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = num_heads\n",
        "        self.d_head = dim//num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.r = r\n",
        "        self.qkv = nn.Linear(dim, dim*3, bias=False)\n",
        "        self.lin = nn.Conv2d(dim, dim, 1)\n",
        "        self.rope = RoPE(head_dim, seq_len=512, base=10000)\n",
        "        self.scale = head_dim**-.5 # v1\n",
        "        # v2\n",
        "        # self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1, 1))), requires_grad=True)\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_heads, 1))), requires_grad=True)\n",
        "        # self.logit_scale = nn.Parameter(10 * torch.ones((num_heads, 1, 1)))\n",
        "\n",
        "        self.downsampler = DownSampler(dim, r=r)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w]\n",
        "        # b, _, h, w = x.size()\n",
        "        x = self.downsampler(x) # [b, r^2, h/r*w/r, c] = [b, r^2, N, c] #  # [b*r^2, h/r*w/r, c]?\n",
        "\n",
        "        q,k,v = self.qkv(x).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k, v = q.unflatten(-1, (self.heads,-1)), k.unflatten(-1, (self.heads,-1)), v.unflatten(-1, (self.heads,-1)) # [b*r^2, h/r*w/r, n_heads, d_head]?\n",
        "        # q,k,v = self.qkv(x).unflatten(-1, (self.heads,-1)).chunk(3, dim=-1) # [b, r^2, h/r*w/r, dim] # [b*r^2, h/r*w/r, dim]?\n",
        "        q, k = self.rope(q), self.rope(k)\n",
        "\n",
        "        q, k = q.softmax(dim=-1)*self.scale, k.softmax(dim=-2)\n",
        "        context = k.transpose(-2,-1) @ v # [batch, n_heads, d_head, d_head]\n",
        "        x = q @ context # [batch, n_heads, T/num_tok, d_head]\n",
        "\n",
        "        # attn = (q @ k.transpose(-2, -1)) * self.scale # v1 attention\n",
        "        # attn = (F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(-2, -1)) * torch.clamp(self.logit_scale, max=4.6052).exp() # v2 attention\n",
        "        # # attn = attn * torch.clamp(self.logit_scale, max=100)\n",
        "        # x = attn.softmax(dim=-1) @ v\n",
        "\n",
        "        x = F.pixel_shuffle(x.flatten(2).permute(2,0,1).unflatten(-1, (h//self.r, w//self.r)), 2).transpose(0,1) # [b*r^2, h/r*w/r, n_heads, d_head] -> [d, b*r^2, h/r,w/r] -> [b,d,h,w]\n",
        "        return self.lin(x)\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, dim, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         d_model = dim*ff_mult\n",
        "#         self.project_in = nn.Sequential(nn.Conv2d(dim, d_model, kernel_size=1), nn.GELU())\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             # nn.Conv2d(d_model, d_model, 5, 1, 5//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 3, 1, 3//2, groups=d_model),\n",
        "#             nn.Conv2d(d_model, d_model, 1, 1, 1//2, groups=d_model),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.Conv2d(d_model, d_model//2, kernel_size=1),\n",
        "#             nn.Conv2d(d_model//2, dim, kernel_size=1))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.project_in(x) # no\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# class SEBlock(nn.Module): # https://github.com/DingXiaoH/RepVGG/blob/main/se_block.py#L7\n",
        "#     def __init__(self, in_ch, dim=None):\n",
        "#         super().__init__()\n",
        "#         dim = dim or in_ch//4\n",
        "#         self.se = nn.Sequential( # [b,c,h,w] -> [b,c,1,1]\n",
        "#             nn.AdaptiveAvgPool2d((1,1)),\n",
        "#             nn.Conv2d(in_ch, dim, 1, 1), nn.ReLU(),\n",
        "#             nn.Conv2d(dim, in_ch, 1, 1), nn.Sigmoid(),\n",
        "#         )\n",
        "#     def forward(self, x): # [b,c,h,w]\n",
        "#         return x * self.se(x) # [b,c,h,w]\n",
        "\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py\n",
        "# # # https://github.com/DingXiaoH/RepVGG/blob/main/repvggplus.py#L28\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, in_ch, out_ch=None, ff_mult=1):\n",
        "#         super().__init__()\n",
        "#         # d_model = dim*ff_mult\n",
        "#         out_ch = out_ch or in_ch\n",
        "#         self.dwconv = nn.ModuleList([\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, 1, 3//2), nn.BatchNorm2d(out_ch)),\n",
        "#             nn.Sequential(nn.Conv2d(in_ch, out_ch, 1, 1, 1//2), nn.BatchNorm2d(out_ch)),\n",
        "#         ])\n",
        "#         self.project_out = nn.Sequential(nn.GELU(), SEBlock(out_ch, out_ch//4)) # act, SEblock\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + sum([conv(x) for conv in self.dwconv])\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "# # me\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, ff_mult=1):\n",
        "        super().__init__()\n",
        "        # d_model = dim*ff_mult\n",
        "        self.project_in = nn.Sequential(nn.BatchNorm2d(d_model), nn.GELU())\n",
        "        self.dwconv = nn.ModuleList([\n",
        "            nn.Conv2d(d_model, d_model, 3, 1, 3//2),\n",
        "            nn.Conv2d(d_model, d_model, 1, 1, 1//2),\n",
        "        ])\n",
        "        # self.project_out = nn.Sequential(nn.GELU(), nn.Conv2d(d_model, dim, kernel_size=1)) # act, SEblock\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.project_in(x) # no\n",
        "        x = x + sum([conv(h) for conv in self.dwconv])\n",
        "        # x = self.project_out(out)\n",
        "        return x\n",
        "\n",
        "class TimestepEmbedder(nn.Module):\n",
        "    def __init__(self, hidden_size, emb_dim=256):\n",
        "        super().__init__()\n",
        "        self.rot_emb = RotEmb(emb_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_dim, hidden_size), nn.SiLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "        )\n",
        "        # nn.init.normal_(self.mlp.weight, std=0.02)\n",
        "        self.mlp.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.normal_(module.weight, std=0.02)\n",
        "    def forward(self, t):\n",
        "        t_freq = self.rot_emb(t)\n",
        "        t_emb = self.mlp(t_freq)\n",
        "        return t_emb\n",
        "\n",
        "\n",
        "class U_DiTBlock(nn.Module):\n",
        "    \"\"\"A IPT block with adaptive layer norm zero (adaLN-Zero) conIPTioning.\"\"\"\n",
        "    def __init__(self, hidden_size, num_heads, down_factor=2):\n",
        "        super().__init__()\n",
        "        self.norm1 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm1 = LayerNorm2d(hidden_size)\n",
        "        self.attn = DownSample_Attn(hidden_size, num_heads=num_heads, r=down_factor)\n",
        "        # self.attn = AttentionBlock(d_model=hidden_size, d_head=hidden_size//num_heads)\n",
        "        self.norm2 = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm2 = LayerNorm2d(hidden_size)\n",
        "        self.mlp = FeedForward(hidden_size)\n",
        "        self.hidden_size = hidden_size\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 6 * hidden_size))\n",
        "        )\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        # shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond).chunk(6, dim=1)\n",
        "        # x = x + gate_msa[...,None,None] * self.attn(modulate(self.norm1(x), shift_msa, scale_msa))\n",
        "        # x = x + gate_mlp[...,None,None] * self.mlp(modulate(self.norm2(x), shift_mlp, scale_mlp))\n",
        "\n",
        "        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = self.adaLN_modulation(cond)[...,None,None].chunk(6, dim=1)\n",
        "        x = x + gate_msa * self.attn((1 + scale_msa) * self.norm(x) + shift_msa)\n",
        "        x = x + gate_mlp * self.mlp((1 + scale_mlp) * self.norm(x) + shift_mlp)\n",
        "\n",
        "        return x\n",
        "\n",
        "def modulate(x, shift, scale):\n",
        "    return x * (1 + scale[...,None,None]) + shift[...,None,None]\n",
        "\n",
        "\n",
        "class FinalLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, out_channels):\n",
        "        super().__init__()\n",
        "        self.in_proj = nn.Conv2d(hidden_size, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        self.adaLN_modulation = nn.Sequential(\n",
        "            nn.SiLU(), zero_module(nn.Linear(hidden_size, 2*hidden_size))\n",
        "        )\n",
        "        self.norm_final = LayerNorm2d(hidden_size, elementwise_affine=False, eps=1e-6)\n",
        "        # self.norm_final = LayerNorm2d(hidden_size)\n",
        "        self.out_proj = zero_module(nn.Conv2d(hidden_size, out_channels, kernel_size=3, stride=1, padding=1))\n",
        "\n",
        "    def forward(self, x, c):\n",
        "        shift, scale = self.adaLN_modulation(c).chunk(2, dim=1)\n",
        "        x = self.in_proj(x)\n",
        "        x = modulate(self.norm_final(x), shift, scale)\n",
        "        x = self.out_proj(x)\n",
        "        return x\n",
        "\n",
        "class UpDownsample(nn.Module):\n",
        "    def __init__(self, n_feat, r=1):\n",
        "        super().__init__()\n",
        "        if r>1: sample = nn.PixelShuffle(r)\n",
        "        elif r<1: sample = nn.PixelUnshuffle(int(1/r))\n",
        "        else: sample = nn.Identity()\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, int(n_feat*r), 3, 1, 1, bias=False), sample)\n",
        "    def forward(self, x): return self.body(x)\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_heads, depth=1, updown=False, red=False):\n",
        "        super().__init__()\n",
        "\n",
        "        if updown=='down': level = [\n",
        "            UpDownsample(hidden_size, r=1/2),\n",
        "            *[U_DiTBlock(hidden_size*2, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        elif updown=='up': level = [\n",
        "            # nn.Conv2d(hidden_size, hidden_size//2, kernel_size=1),\n",
        "            # *[U_DiTBlock(hidden_size//2, num_heads) for _ in range(depth)],\n",
        "            # UpDownsample(hidden_size//2, r=2)\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)],\n",
        "            UpDownsample(hidden_size, r=2)\n",
        "            ]\n",
        "        else: level = [\n",
        "            nn.Conv2d(hidden_size*2, hidden_size, kernel_size=1) if red else nn.Identity(),\n",
        "            *[U_DiTBlock(hidden_size, num_heads) for _ in range(depth)]\n",
        "        ]\n",
        "        self.seq = Seq(*level)\n",
        "\n",
        "    def forward(self, x, cond):\n",
        "        return self.seq(x, cond)\n",
        "\n",
        "\n",
        "class U_DiT(nn.Module):\n",
        "    \"\"\"Diffusion UNet model with a Transformer backbone.\"\"\"\n",
        "    def __init__(self, in_channels=3, hidden_size=96, depth=[2,5,8,5,2], num_heads=16, cond_dim=16):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = in_channels #* 2 if learn_sigma else in_channels\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.x_embedder = nn.Conv2d(in_channels, hidden_size, kernel_size=3, stride=1, padding=1)\n",
        "        # Initialize patch_embed like nn.Linear (instead of nn.Conv2d):\n",
        "        w = self.x_embedder.weight.data\n",
        "        nn.init.xavier_uniform_(w.view([w.shape[0], -1]))\n",
        "        nn.init.constant_(self.x_embedder.bias, 0)\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.t_embedder = TimestepEmbedder(hidden_size*(1+2+4))\n",
        "        self.y_embedder = nn.Linear(cond_dim, hidden_size*(1+2+4))\n",
        "\n",
        "        self.down_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown=False),\n",
        "            levelBlock(hidden_size, num_heads, depth=1, updown='down'),\n",
        "        ])\n",
        "        self.middle_block = Seq(\n",
        "            UpDownsample(hidden_size*2, r=1/2),\n",
        "            # levelBlock(hidden_size*4, num_heads, depth=1, updown=False),\n",
        "            *[U_DiTBlock(hidden_size*4, num_heads) for _ in range(depth[2])],\n",
        "            UpDownsample(hidden_size*4, r=2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([\n",
        "            levelBlock(hidden_size*2, num_heads, depth=1, updown='up'),\n",
        "            levelBlock(hidden_size*1, num_heads, depth=1, updown=False, red=True),\n",
        "        ])\n",
        "\n",
        "        self.final_layer = FinalLayer(hidden_size, self.out_channels)\n",
        "\n",
        "        self.apply(self._basic_init)\n",
        "    def _basic_init(self, module):\n",
        "        if isinstance(module, nn.Linear) or isinstance(module, nn.Conv2d):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.constant_(module.bias, 0)\n",
        "\n",
        "\n",
        "    def forward(self, x, t, y): # [b,c,h,w], time [b], class label [b]\n",
        "        x = self.x_embedder(x) # (N, C, H, W)\n",
        "        c123 = self.t_embedder(t) + self.y_embedder(y)\n",
        "        cond = c123.split([self.hidden_size, self.hidden_size*2, self.hidden_size*4], dim=1)\n",
        "\n",
        "        blocks = []\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, cond[i])\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, cond[-1])\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, cond[-2-i]) # x = up(x, blocks[-i - 1])\n",
        "\n",
        "        x = self.final_layer(x, cond[0]) # (N, T, patch_size ** 2 * out_channels)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# def U_DiT_S(**kwargs):\n",
        "#     return U_DiT(down_factor=2, hidden_size=96, num_heads=4, depth=[2,5,8,5,2], mlp_ratio=2, downsampler='dwconv5', down_shortcut=1)\n",
        "# def U_DiT_B:  U_DiT(hidden_size=192, num_heads=8,\n",
        "# def U_DiT_L: U_DiT(hidden_size=384, num_heads=16,\n",
        "\n",
        "cond_dim=10\n",
        "model = U_DiT(in_channels=3, hidden_size=16, num_heads=4, depth=[2,3,3,3,2], cond_dim=cond_dim).to(device)\n",
        "\n",
        "batch=64\n",
        "# inputs = torch.rand(batch, 3, 32, 32)\n",
        "inputs = torch.rand((batch, 3, 64, 64), device=device)\n",
        "t = torch.rand((batch), device=device)\n",
        "y = torch.rand((batch, cond_dim), device=device)\n",
        "\n",
        "# model(inputs, t, y)\n",
        "# out = model(inputs, t, y)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) #\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "\n",
        "\n",
        "# model.train()\n",
        "# out = model(inputs, t, y)\n",
        "# gt = torch.rand(1, 8, 32, 32)\n",
        "# loss = torch.mean(out-gt)\n",
        "# loss.backward()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SlxB9mFM6abU"
      },
      "outputs": [],
      "source": [
        "# @title unet me\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, emb_dim=None, drop=0.):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch=in_ch\n",
        "        act = nn.SiLU() #\n",
        "        self.block1 = nn.Sequential(nn.BatchNorm2d(in_ch), act, nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "        self.block2 = Seq(nn.BatchNorm2d(out_ch), scale_shift(out_ch, emb_dim) if emb_dim != None else nn.Identity(), act, nn.Conv2d(out_ch, out_ch, 3, padding=1))\n",
        "        self.res_conv = nn.Conv2d(in_ch, out_ch, 1) if in_ch != out_ch else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb=None): # [b,c,h,w], [batch, emb_dim]\n",
        "        h = self.block1(x)\n",
        "        h = self.block2(h, emb)\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb): # [b,c,h,w], [b,emb_dim]\n",
        "        scale, shift = self.time_mlp(emb)[..., None, None].chunk(2, dim=1) # [b,t_dim]->[b,2*x_dim,1,1]->[b,x_dim,1,1]\n",
        "        return x * (scale + 1) + shift\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, emb_dim, cond_dim, n_head=None, d_head=8, updown=False, r=2):\n",
        "        super().__init__()\n",
        "        if updown=='down': in_ch = in_ch*r**2\n",
        "        elif updown=='up': out_ch = out_ch*r**2\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            nn.PixelUnshuffle(r) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, out_ch, emb_dim=emb_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            nn.PixelShuffle(r) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.d_model = d_model # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = d_model // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(d_model)\n",
        "        emb_dim = d_model# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(d_model, emb_dim), nn.SiLU(), nn.Linear(emb_dim, emb_dim))\n",
        "\n",
        "\n",
        "        tok_dim = d_model\n",
        "        # if tok_dim == None: tok_dim = d_model\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.LayerNorm(cond_dim), nn.Linear(cond_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "        cond_dim = tok_dim\n",
        "\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, d_model, 3, padding=1), act)\n",
        "        # self.init_conv = CrossEmbedLayer(in_ch, dim_out=d_model, kernel_sizes=(3, 7, 15), stride=1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7//2)\n",
        "\n",
        "        dim_mults = [1,2,3,4,4] # [1,2,3,4] [1,2,2,2]\n",
        "        ch_list = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "        # in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        # for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "\n",
        "\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], emb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = ch_list[-1]*2**2 # 512\n",
        "        self.middle_block = Seq(\n",
        "            nn.PixelUnshuffle(2), ResBlock(ch, ch, emb_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, ch, emb_dim), nn.PixelShuffle(2),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], emb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "        # for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(d_model), nn.SiLU(), nn.Conv2d(d_model, out_ch, 3, padding=1)) # zero\n",
        "        # self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3//2) # lucid; or prepend final res block\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        # t_emb = self.rotemb(t)\n",
        "        # emb = self.time_emb(t_emb) #+ self.label_emb(y) # class conditioning nn.Embedding(num_classes, emb_dim)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        t_hid, t_tok = self.time_mlp(t).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        c_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        emb = t_hid + c_hid # [batch, emb_dim]\n",
        "        cond = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        cond = self.norm_cond(cond)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1) # scale residuals by 1/sqrt2\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, d_model=16, cond_dim=cond_dim, depth=3).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# x=torch.rand((batch,3,16,16),device=device)\n",
        "x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# print(t)\n",
        "cond=torch.rand((batch,cond_dim),device=device)\n",
        "# [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "print(x.shape,t.shape,cond.shape)\n",
        "out = model(x, t, cond)\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkVBxwyjTpM-"
      },
      "outputs": [],
      "source": [
        "# @title conv deconv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            PixelShuffleConv(in_ch, d_list[0], k_list[0], r=1/2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            nn.Dropout2d(drop), PixelShuffleConv(d_list[0], d_list[1], k_list[1], r=1/2), act,\n",
        "        )\n",
        "    def forward(self, x): return self.conv(x) # [batch, 3,64,64] -> [batch, c,h,w]\n",
        "\n",
        "class Deconv(nn.Module):\n",
        "    def __init__(self, out_ch=3, d_list=[32, 64], k_list=[7,5], act=nn.GELU(), drop=0.): # ReLU GELU SiLU\n",
        "        super().__init__()\n",
        "        self.deconv = nn.Sequential(\n",
        "            PixelShuffleConv(d_list[1], d_list[0], k_list[1], r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            PixelShuffleConv(d_list[0], out_ch, k_list[0], r=2),\n",
        "        )\n",
        "    def forward(self, x): return self.deconv(x) # [batch, c,h,w] -> [batch, 3,64,64]\n",
        "\n",
        "\n",
        "class PixelAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, d_model=256, out_ch=None, kernels=[7,5], mult=[1]):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.d_model, self.out_ch = in_ch, d_model, out_ch\n",
        "        d_list=[d_model*m for m in mult]\n",
        "        in_list, out_list = [in_ch, *d_list[:-1]], [*d_list[:-1], out_ch]\n",
        "        act = nn.GELU() # ReLU GELU SiLU\n",
        "        self.encoder = nn.Sequential( # nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0), # SiLU ReLU\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=1/2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act,) for i, (in_dim, out_dim, kernel) in enumerate(zip(in_list, out_list, kernels))], # conv,norm,act except for last layer: no norm\n",
        "            # PixelShuffleConvDown(in_ch, d_list[0], 7, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvDown(d_list[0], d_list[1], 5, r=2), act,\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            # *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), nn.BatchNorm2d(out_dim) if i!=len(d_list) else nn.Identity(), act if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            *[nn.Sequential(PixelShuffleConv(in_dim, out_dim, kernel, r=2), *(nn.BatchNorm2d(out_dim), act) if i!=len(d_list) else nn.Identity()) for i, (in_dim, out_dim, kernel) in enumerate(zip(reversed(out_list), reversed(in_list), reversed(kernels)))], # conv,norm,act except for last layer: only conv\n",
        "            # PixelShuffleConvUp(d_list[1], d_list[0], 5, r=2), nn.BatchNorm2d(d_list[0]), act,\n",
        "            # PixelShuffleConvUp(d_list[0], in_ch, 7, r=2), act,\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.encode(x)\n",
        "        x = self.decode(x)\n",
        "        return x\n",
        "    def encode(self, x): return self.encoder(x)\n",
        "    def decode(self, x): return self.decoder(x)\n",
        "\n",
        "\n",
        "# in_ch=3\n",
        "# model_ch=16\n",
        "# d_list=[16, 16] # [16,32]\n",
        "# k_list=[7,5] # [7,5]\n",
        "# conv = Conv(in_ch=in_ch, d_list=d_list, k_list=k_list, act=nn.ReLU()) # ReLU GELU SiLU\n",
        "\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# conv = Conv().to(device)\n",
        "# print(sum(p.numel() for p in conv.parameters() if p.requires_grad)) # 19683\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# enc = conv(input)\n",
        "# print(enc.shape)\n",
        "# out = deconv(enc)\n",
        "# print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GMFVOgT4sJ-y"
      },
      "outputs": [],
      "source": [
        "# @title stable diffusion unet next\n",
        "# https://github.com/CompVis/latent-diffusion/blob/main/ldm/modules/diffusionmodules/openaimodel.py#L413\n",
        "# is from https://github.com/openai/guided-diffusion/blob/main/guided_diffusion/unet.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# torch.set_default_dtype(torch.float16)\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch, 3, padding=1) # og\n",
        "        self.conv = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, padding=1), act)\n",
        "\n",
        "    def forward(self, x): # [N,C,...]\n",
        "        x = F.interpolate(x, scale_factor=2, mode=\"nearest\") # if self.conv_dim == 3: x = F.interpolate(x, (x.shape[2], x.shape[3] * 2, x.shape[4] * 2), mode=\"nearest\")\n",
        "        x = self.conv(x) # optional\n",
        "        return x\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.op = nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1) # optional # stride = 2 if conv_dim != 3 else (1, 2, 2) # If 3D, then downsampling occurs in the inner-two dimensions\n",
        "        self.op = nn.Sequential(nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1), act)\n",
        "        # self.op = avg_pool_nd(conv_dim, kernel_size=stride, stride=stride) # alternative\n",
        "\n",
        "    def forward(self, x): # [N,C,*spatial]\n",
        "        return self.op(x)\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, temb_dim, out_ch=None, scale_shift=False, updown=False, drop=0.):\n",
        "        super().__init__()\n",
        "        self.temb_dim = temb_dim # number of timestep embedding channels\n",
        "        if out_ch == None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch = in_ch, out_ch\n",
        "        self.scale_shift = scale_shift\n",
        "        if updown=='up': self.h_upd, self.x_upd = Upsample(in_ch), Upsample(in_ch)\n",
        "        elif updown=='down': self.h_upd, self.x_upd = Downsample(in_ch), Downsample(in_ch)\n",
        "        else: self.h_upd = self.x_upd = nn.Identity()\n",
        "        self.in_layers = nn.Sequential(nn.BatchNorm2d(in_ch), nn.SiLU(), self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1),) # zero\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "        # self.in_layers = nn.Sequential(self.h_upd, nn.Conv2d(in_ch, out_ch, 3, padding=1), nn.BatchNorm2d(out_ch), nn.SiLU(), self.h_upd, nn.Conv2d(out_ch, out_ch, 3, padding=1)) # no bn before FiLM\n",
        "\n",
        "        # Conv bn film act res\n",
        "        self.emb_layers = nn.Sequential(nn.SiLU(), nn.Linear(temb_dim, 2 * out_ch if scale_shift else out_ch),)\n",
        "        # self.emb_layers = nn.Sequential(nn.Linear(temb_dim, out_ch), nn.SiLU(), nn.Linear(out_ch, 2 * out_ch if scale_shift else out_ch),)\n",
        "        self.out_layers = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            # nn.BatchNorm2d(out_ch), nn.SiLU(), nn.Dropout(drop), zero_module(nn.Conv2d(out_ch, out_ch, 3, padding=1)),\n",
        "        )\n",
        "\n",
        "        if out_ch == in_ch: self.skip = nn.Identity() # no need to change chanels\n",
        "        else:\n",
        "            # self.skip = nn.Conv2d(in_ch, out_ch, 3, padding=1) # spatial convolution to change the channels in the skip connection\n",
        "            self.skip = nn.Conv2d(in_ch, out_ch, 1) # smaller 1x1 convolution to change the channels in the skip connection\n",
        "\n",
        "    def forward(self, x, emb): # [N, C, ...], [N, temb_dim]\n",
        "        # print(\"res fwd x\", x.shape, self.in_ch, self.out_ch)\n",
        "        h = self.in_layers(x) # norm, act, h_upd, conv\n",
        "        x = self.x_upd(x)\n",
        "        emb_out = self.emb_layers(emb) # act, lin\n",
        "        # print(\"res fwd h emb_out\", h.shape, emb_out.shape)\n",
        "        while len(emb_out.shape) < len(h.shape): emb_out = emb_out[..., None]\n",
        "        if self.scale_shift: # FiLM\n",
        "            out_norm, out_rest = self.out_layers[0], self.out_layers[1:]\n",
        "            scale, shift = torch.chunk(emb_out, 2, dim=1)\n",
        "            h = out_norm(h) * (1 + scale) + shift\n",
        "            h = out_rest(h) # act, drop, conv\n",
        "        else:\n",
        "            h = h + emb_out\n",
        "            h = self.out_layers(h)\n",
        "        return h + self.skip(x) # [N, C, ...]\n",
        "\n",
        "\n",
        "class levelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, temb_dim, cond_dim, n_head=None, d_head=8, updown=False, *args):\n",
        "        super().__init__()\n",
        "        if n_head==None: n_head = out_ch // d_head\n",
        "        layers = [\n",
        "            # Downsample(in_ch, out_ch) if updown=='down' else nn.Identity(),\n",
        "            Downsample(in_ch) if updown=='down' else nn.Identity(),\n",
        "            ResBlock(in_ch, temb_dim, out_ch=out_ch),\n",
        "            # SpatialTransformer(out_ch, n_head, d_head, depth=1, cond_dim=cond_dim),\n",
        "            AttentionBlock(out_ch, d_head, cond_dim),\n",
        "            Upsample(out_ch) if updown=='up' else nn.Identity(),\n",
        "            # Upsample(in_ch, out_ch) if updown=='up' else nn.Identity(),\n",
        "            ]\n",
        "        self.seq = Seq(*layers)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        return self.seq(x, emb, cond)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_ch=3, model_ch=16, out_ch=None, cond_dim=16, depth=4, num_res_blocks=1, n_head=-1, d_head = 4):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        self.model_ch = model_ch # base channel count for the model\n",
        "        out_ch = out_ch or in_ch\n",
        "        n_head = model_ch // d_head\n",
        "\n",
        "        self.rotemb = RotEmb(model_ch)\n",
        "        temb_dim = model_ch# * 4\n",
        "        self.time_emb = nn.Sequential(nn.Linear(model_ch, temb_dim), nn.SiLU(), nn.Linear(temb_dim, temb_dim))\n",
        "\n",
        "        self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1))\n",
        "        # self.in_block = nn.Sequential(nn.Conv2d(in_ch, model_ch, 3, padding=1), act)\n",
        "\n",
        "        ch_list = [model_ch*2**i for i in range(depth+1)] # [32, 64, 128, 256]\n",
        "        self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown=None if i==0 else 'down') for i in range(depth)])\n",
        "        # self.down_list = nn.ModuleList([levelBlock(ch_list[i], ch_list[i+1], temb_dim, cond_dim, updown='down') for i in range(depth)])\n",
        "\n",
        "        ch = 2*ch_list[-1] # 512\n",
        "        self.middle_block = Seq(\n",
        "            Downsample(ch_list[-1]),\n",
        "            ResBlock(ch_list[-1], temb_dim, ch),\n",
        "            # SpatialTransformer(ch, ch//d_head, d_head, cond_dim=cond_dim),\n",
        "            AttentionBlock(ch, d_head, cond_dim),\n",
        "            ResBlock(ch, temb_dim, ch_list[-1]),\n",
        "            Upsample(ch_list[-1]),\n",
        "        )\n",
        "        self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown=None if i==0 else 'up') for i in reversed(range(depth))])\n",
        "        # self.up_list = nn.ModuleList([levelBlock(2*ch_list[i+1], ch_list[i], temb_dim, cond_dim, updown='up') for i in reversed(range(depth))])\n",
        "\n",
        "        self.out_block = nn.Sequential(nn.BatchNorm2d(model_ch), nn.SiLU(), nn.Conv2d(model_ch, out_ch, 3, padding=1)) # zero\n",
        "\n",
        "    def forward(self, x, t=None, cond=None): # [N, c,h,w], [N], [N, cond_dim]\n",
        "        t_emb = self.rotemb(t)\n",
        "        # t_emb = timestep_embedding(t, self.model_ch, repeat_only=False)\n",
        "        emb = self.time_emb(t_emb)\n",
        "        # emb = emb + self.label_emb(y) # class conditioning nn.Embedding(num_classes, temb_dim)\n",
        "        blocks = []\n",
        "        x = self.in_block(x)\n",
        "        for i, down in enumerate(self.down_list):\n",
        "            x = down(x, emb, cond)\n",
        "            blocks.append(x)\n",
        "        x = self.middle_block(x, emb, cond)\n",
        "        for i, up in enumerate(self.up_list):\n",
        "            # print(\"unet fwd\", x.shape,blocks[-i-1].shape)\n",
        "            x = torch.cat([x, blocks[-i-1]*2**.5], dim=1)\n",
        "            x = up(x, emb, cond) # x = up(x, blocks[-i - 1])\n",
        "        return self.out_block(x)\n",
        "\n",
        "\n",
        "\n",
        "# 64,64 -vae-> 16,16 -unet->\n",
        "batch = 4\n",
        "cond_dim=10\n",
        "model = UNet(in_ch=1, model_ch=16, cond_dim=cond_dim, depth=4).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "# print(model)\n",
        "\n",
        "# # x=torch.rand((batch,3,16,16),device=device)\n",
        "# x=torch.rand((batch,1,16,16),device=device)\n",
        "# y=torch.rand((batch,1,16,16),device=device)\n",
        "# t = torch.rand((batch,), device=device) # in [0,1] [N]\n",
        "# # print(t)\n",
        "# cond=torch.rand((batch,cond_dim),device=device)\n",
        "# # [2, 1, 16, 16]) torch.Size([2]) torch.Size([2, 10]\n",
        "# print(x.shape,t.shape,cond.shape)\n",
        "# out = model(x, t, cond)\n",
        "# print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n",
        "# optim = torch.optim.SGD(model.parameters(), lr=1e-4) # 1e-3 3e-3\n",
        "\n",
        "cond_emb = nn.Embedding(10, cond_dim).to(device)\n",
        "# for name, param in model.named_parameters(): print(name, param)\n",
        "# optim.zero_grad()\n",
        "# loss = F.mse_loss(out, y)\n",
        "# loss.backward()\n",
        "# optim.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "F5znfg9g-BOQ"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen stuff\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import inspect\n",
        "class Seq(nn.Sequential):\n",
        "    def __init__(self, *args):\n",
        "        super().__init__(*args)\n",
        "        for layer in self:\n",
        "            params = inspect.signature(layer.forward).parameters.keys()\n",
        "            layer._fwdparams = ','.join(params)\n",
        "\n",
        "    def forward(self, x, emb=None, cond=None):\n",
        "        for layer in self:\n",
        "            args = [x]\n",
        "            if 'emb' in layer._fwdparams: args.append(emb)\n",
        "            if 'cond' in layer._fwdparams: args.append(cond)\n",
        "            x = layer(*args)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ChanRMSNorm(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.scale = dim ** 0.5\n",
        "        self.gamma = nn.Parameter(torch.ones(dim, 1, 1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return F.normalize(x, dim = 1) * self.scale * self.gamma\n",
        "\n",
        "\n",
        "class Parallel(nn.Module):\n",
        "    def __init__(self, *fns):\n",
        "        super().__init__()\n",
        "        self.fns = nn.ModuleList(fns)\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [fn(x) for fn in self.fns]\n",
        "        return sum(outputs)\n",
        "\n",
        "class GlobalContext(nn.Module): # Global Context (GC) block\n",
        "    \"\"\" basically a superior form of squeeze-excitation that is attention-esque \"\"\"\n",
        "    def __init__(self, dim_in, dim_out):\n",
        "        super().__init__()\n",
        "        self.to_k = nn.Conv2d(dim_in, 1, 1)\n",
        "        hidden_dim = max(3, dim_out // 2)\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Conv2d(dim_in, hidden_dim, 1), nn.SiLU(),\n",
        "            nn.Conv2d(hidden_dim, dim_out, 1), nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # [b, c, h, w]\n",
        "        context = self.to_k(x)\n",
        "        x, context = x.flatten(2), context.flatten(2) # [b, c, h*w] ,[b,1,h*w]\n",
        "        out = x @ context.softmax(dim=-1).transpose(1,2) # [b, c, 1]\n",
        "        out = out.unsqueeze(-1) # [b, c, 1, 1]\n",
        "        return self.net(out) # [b, dim_out, 1, 1]\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, d_head = 64,n_heads = 8, context_dim = None, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        d_model = d_head * n_heads\n",
        "        self.d_model, self.n_heads, self.d_head = d_model, n_heads, d_head\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, d_head))\n",
        "        self.qkv = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model + 2*d_head, bias=False))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "\n",
        "        self.to_context = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, 2*d_head)) if context_dim!=None else None\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim),)\n",
        "\n",
        "    def forward(self, x, cond = None, mask = None, attn_bias = None): # [batch, T, d_model] = [b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        q, k, v = self.qkv(x).split([self.d_model, self.d_head, self.d_head], dim=-1) # [batch, T, d_model], 2*[batch, T, d_head]\n",
        "        q = q.reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,-1,-1)).unbind(dim=1) # [2,1,d_head]->[batch,2,1,d_head]->[batch,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, 1+T, d_head]\n",
        "\n",
        "        if self.to_context!=None:\n",
        "            ck, cv = self.to_context(cond).chunk(2, dim = -1) # [batch, num_tok, d_head]\n",
        "            k, v = torch.cat((ck, k), dim=-2), torch.cat((cv, v), dim=-2) # [batch, num_tok+1+T, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "        k, v = k.unsqueeze(1), v.unsqueeze(1) # [batch, 1, num_tok+1+T, d_head]\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, num_tok+1+T]\n",
        "        if attn_bias!=None: sim = sim + attn_bias # relative positional encoding (T5 style)\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    def __init__(self, dim, context_dim = None, d_head = 64,n_heads = 8, scale = 8):\n",
        "        super().__init__()\n",
        "        self.scale = scale\n",
        "        self.n_heads =n_heads\n",
        "        d_model = d_head *n_heads\n",
        "        if context_dim==None: context_dim = dim\n",
        "        self.to_q = nn.Sequential(nn.LayerNorm(dim), nn.Linear(dim, d_model, bias=False))\n",
        "        self.to_kv = nn.Linear(context_dim, d_model * 2, bias=False)\n",
        "        # self.to_kv = nn.Sequential(nn.LayerNorm(context_dim), nn.Linear(context_dim, d_model * 2, bias=False))\n",
        "        self.null_kv = nn.Parameter(torch.randn(2, 1, 1, d_head))\n",
        "        self.q_scale, self.k_scale = nn.Parameter(torch.ones(d_head)), nn.Parameter(torch.ones(d_head))\n",
        "        self.to_out = nn.Sequential(nn.Linear(d_model, dim, bias = False), nn.LayerNorm(dim))\n",
        "\n",
        "    def forward(self, x, cond, mask = None): # [batch, T, dim]=[b, hw, c], [batch, num_tok, context_dim]\n",
        "        batch, T, _ = x.shape#[0]\n",
        "        _, num_tok, _ = cond.shape\n",
        "        q = self.to_q(x).reshape(batch, T, self.n_heads, -1).transpose(1,2) # [batch, n_heads, T, d_head]\n",
        "        k, v = self.to_kv(cond).chunk(2, dim = -1) # [batch, num_tok, d_model]\n",
        "        k = k.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "        v = v.reshape(batch, num_tok, self.n_heads, -1).transpose(1,2) # [batch, n_heads, num_tok, d_head]\n",
        "\n",
        "        nk, nv = self.null_kv.expand((batch,-1,self.n_heads,-1,-1)).unbind(dim=1) # [2,1,1,d_head]->[batch,2,n_heads,1,d_head]->[batch,n_heads,1,d_head]\n",
        "        k, v = torch.cat((nk, k), dim=-2), torch.cat((nv, v), dim=-2) # [batch, n_heads, 1+num_tok, d_head]\n",
        "\n",
        "        q, k = F.normalize(q, dim=-1), F.normalize(k, dim=-1) # qk rmsnorm\n",
        "        q, k = q * self.q_scale, k * self.k_scale\n",
        "\n",
        "        sim = q @ k.transpose(-2,-1) * self.scale # [batch, n_heads, T, 1+num_tok]\n",
        "        if mask!=None:\n",
        "            mask = F.pad(mask, (1, 0), value=True).unsqueeze(1).unsqeeze(2) # [b j] -> [b 1 1 j]\n",
        "            sim = sim.masked_fill(~mask, -torch.finfo(sim.dtype).max)\n",
        "        attn = sim.softmax(dim=-1) # attn = sim.softmax(dim=-1, dtype=torch.float32).to(sim.dtype)\n",
        "        out = (attn @ v).transpose(1,2).flatten(2) # [batch, n_heads, T, d_head] -> [batch, T, d_model]\n",
        "        return self.to_out(out)\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, dim_in, kernel_sizes, dim_out = None, stride = 2):\n",
        "        super().__init__()\n",
        "        assert all([*map(lambda t: (t % 2) == (stride % 2), kernel_sizes)])\n",
        "        if dim_out==None: dim_out = dim_in\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "        dim_scales = [int(dim_out / (2 ** i)) for i in range(1, len(kernel_sizes))] # [64, 32]\n",
        "        dim_scales = [*dim_scales, dim_out - sum(dim_scales)] # [64, 32, 32]\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of dim_out; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(dim_in, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "\n",
        "\n",
        "def Upsample(in_ch, out_ch=None):\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.Interpolate(scale_factor = 2, mode = 'nearest'), nn.Conv2d(in_ch, out_ch, 3, padding=1))\n",
        "\n",
        "class PixelShuffleUpsample(nn.Module):\n",
        "    \"\"\"code shared by @MalumaDev at DALLE2-pytorch for addressing checkboard artifacts https://arxiv.org/ftp/arxiv/papers/1707/1707.02937.pdf\"\"\"\n",
        "    def __init__(self, in_ch, out_ch=None):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * 4, 1), nn.SiLU(), nn.PixelShuffle(2)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        self.init_conv_(self.net[0])\n",
        "\n",
        "    def init_conv_(self, conv):\n",
        "        o, i, h, w = conv.weight.shape\n",
        "        conv_weight = torch.empty(o//4, i, h, w)\n",
        "        nn.init.kaiming_uniform_(conv_weight)\n",
        "        conv_weight = conv_weight.repeat(4,1,1,1)\n",
        "        conv.weight.data.copy_(conv_weight)\n",
        "        nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "def Downsample(in_ch, out_ch=None): # https://arxiv.org/abs/2208.03641 shows this is the most optimal way to downsample, named SP-conv in the paper, but basically a pixel unshuffle\n",
        "    if out_ch==None: out_ch = in_ch\n",
        "    return nn.Sequential(nn.PixelUnshuffle(2), nn.Conv2d(in_ch * 4, out_ch, 1)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "\n",
        "\n",
        "def FeedForward(dim, mult = 2):\n",
        "    hidden_dim = int(dim * mult)\n",
        "    return nn.Sequential(\n",
        "        nn.LayerNorm(dim), nn.Linear(dim, hidden_dim, bias = False), nn.GELU(),\n",
        "        nn.LayerNorm(hidden_dim), nn.Linear(hidden_dim, dim, bias = False)\n",
        "    )\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, depth = 1,n_heads = 8, d_head = 32, ff_mult = 2, context_dim = None):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(nn.ModuleList([\n",
        "                Attention(dim = dim,n_heads =n_heads, d_head = d_head, context_dim = context_dim),\n",
        "                FeedForward(dim = dim, mult = ff_mult)\n",
        "            ]))\n",
        "\n",
        "    def forward(self, x, cond = None):\n",
        "        bchw = x.shape\n",
        "        x = x.flatten(2).transpose(1, 2) # [b, c, h, w] -> [b, h*w, c]\n",
        "        for attn, ff in self.layers:\n",
        "            x = attn(x, cond) + x\n",
        "            x = ff(x) + x\n",
        "        x = x.transpose(1, 2).reshape(*bchw) # [b, h*w, c] -> [b, c, h, w]\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResnetBlock(nn.Module):\n",
        "    def __init__(self, dim, dim_out, tok_dim = None, emb_dim = None,n_heads=None, d_head=None):\n",
        "        super().__init__()\n",
        "        if tok_dim != None: self.cross_attn = CrossAttention(dim = dim_out, context_dim = tok_dim, n_heads=n_heads, d_head=d_head) # CrossAttention LinearCrossAttention\n",
        "        else: self.cross_attn = None\n",
        "        self.block1 = nn.Sequential(ChanRMSNorm(dim), nn.SiLU(), nn.Conv2d(dim, dim_out, 3, padding = 1))\n",
        "        self.block2 = Seq(ChanRMSNorm(dim_out), scale_shift(dim_out, emb_dim) if emb_dim != None else nn.Identity(), nn.SiLU(), nn.Conv2d(dim_out, dim_out, 3, padding = 1))\n",
        "        self.gca = GlobalContext(dim_in = dim_out, dim_out = dim_out)\n",
        "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
        "\n",
        "    def forward(self, x, emb = None, cond = None):\n",
        "        h = self.block1(x)\n",
        "        # print('ResnetBlock fwd', h.shape)\n",
        "        if self.cross_attn != None:\n",
        "            bhwc = h.shape\n",
        "            h = h.flatten(2).transpose(1, 2) # [b, h, w, c] -> [b, h*w, c]\n",
        "            h = self.cross_attn(h, cond=cond) + h\n",
        "            h = h.transpose(1, 2).reshape(*bhwc) # [b, h*w, c] -> [b, c, h, w]\n",
        "        h = self.block2(h, emb)\n",
        "        h = h * self.gca(h) # use_gca\n",
        "        return h + self.res_conv(x)\n",
        "\n",
        "\n",
        "class scale_shift(nn.Module): # FiLM\n",
        "    def __init__(self, x_dim, t_dim):\n",
        "        super().__init__()\n",
        "        self.time_mlp = nn.Sequential(nn.SiLU(), nn.Linear(t_dim, x_dim*2),)\n",
        "\n",
        "    def forward(self, x, emb = None): # [b,c,h,w], [b,emb_dim]\n",
        "        # print('scale_shift fwd', x.shape, emb.shape)\n",
        "        emb = self.time_mlp(emb)[..., None, None] # [b,t_dim] -> [b,2*x_dim,1,1]\n",
        "        scale, shift = emb.chunk(2, dim = 1) # [b,x_dim,1,1]\n",
        "        x = x * (scale + 1) + shift\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KpM1jlvFvD9i"
      },
      "outputs": [],
      "source": [
        "# @title lucidrains imagen next\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from einops.layers.torch import Rearrange\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, d_model, # 512 # base dimension of your u-net should ideally be no smaller than 128, as recommended by a professional DDPM trainer https://nonint.com/2022/05/04/friends-dont-let-friends-train-small-diffusion-models/\n",
        "        c_dim = 16, # cond vec dim\n",
        "        tok_dim = None, # token dim\n",
        "        out_dim = None,\n",
        "        dim_mults=(1, 2, 4, 8), # (1, 2, 3, 4)\n",
        "        in_ch = 3, out_ch = None,\n",
        "        layer_attns = True, # (False, True, True, True)\n",
        "        layer_cross_attns = True, # (False, True, True, True)\n",
        "        # num_resnet_blocks = 1, # 3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.in_ch = in_ch\n",
        "        if out_ch == None: self.out_ch = in_ch\n",
        "        d_head = 64\n",
        "        n_heads = 8 # 8 # ideally at least 4 or 8\n",
        "\n",
        "        self.init_conv = CrossEmbedLayer(in_ch, dim_out = d_model, kernel_sizes = (3, 7, 15), stride = 1) #if init_cross_embed else nn.Conv2d(in_ch, d_model, 7, padding = 7 // 2)\n",
        "        dims = [d_model] + [d_model * m for m in dim_mults] # [128, 128, 256, 384, 512]\n",
        "        # dims = [d_model * m for m in dim_mults] # [128, 256, 384, 512]\n",
        "\n",
        "        # time conditioning\n",
        "        if tok_dim == None: tok_dim = d_model\n",
        "\n",
        "        # embedding time for log(snr) noise from continuous version\n",
        "        emb_dim = 16\n",
        "        emb_dim = d_model * 4 #* (2 if lowres_cond else 1)\n",
        "\n",
        "        # pos_emb = RotEmb(emb_dim, top=torch.pi, base=10000)\n",
        "        # self.time_hiddens = nn.Sequential(pos_emb, nn.Linear(emb_dim, emb_dim), nn.SiLU())\n",
        "\n",
        "        # self.time_cond = nn.Sequential(nn.Linear(emb_dim, emb_dim))\n",
        "        # num_time_tokens = 2\n",
        "        # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim * num_time_tokens), Rearrange('b (r d) -> b r d', r = num_time_tokens))\n",
        "        # # self.time_tokens = nn.Sequential(nn.Linear(emb_dim, tok_dim))\n",
        "\n",
        "        self.num_time_tokens = 2\n",
        "        self.time_mlp = nn.Sequential(\n",
        "            RotEmb(emb_dim, top=torch.pi, base=10000),\n",
        "            nn.Linear(emb_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + self.num_time_tokens * tok_dim),\n",
        "        )\n",
        "        self.emb_dim, self.tok_dim = emb_dim, tok_dim\n",
        "\n",
        "\n",
        "        # self.cond_tokens = nn.Linear(c_dim, tok_dim)\n",
        "        # self.cond_cond = nn.Sequential(\n",
        "        #     nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "        #     nn.Linear(emb_dim, emb_dim)\n",
        "        # )\n",
        "        self.cond_mlp = nn.Sequential(\n",
        "            nn.Linear(c_dim, tok_dim),\n",
        "            nn.LayerNorm(tok_dim), nn.Linear(tok_dim, emb_dim), nn.SiLU(),\n",
        "            nn.Linear(emb_dim, emb_dim + tok_dim)\n",
        "        )\n",
        "\n",
        "        self.norm_cond = nn.LayerNorm(tok_dim)\n",
        "\n",
        "        in_out = list(zip(dims[:-1], dims[1:]))\n",
        "        num_layers = len(in_out)\n",
        "        # num_layers = len(dims)\n",
        "\n",
        "        self.downs = nn.ModuleList([])\n",
        "        # skip_connect_dims = [] # keep track of skip connection dimensions\n",
        "        for ind, (dim_in, dim_out) in enumerate(in_out):\n",
        "            is_last = ind >= (num_layers - 1)\n",
        "            # print('down', dim_in, dim_out)\n",
        "\n",
        "            # skip_connect_dims.append(dim_in) # dim_out if memory_efficient\n",
        "            self.downs.append(nn.ModuleList([\n",
        "                # Downsample(dim_in, dim_out), # memory_efficient pre_downsample; self.downs all dim_out # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "                ResnetBlock(dim_in, dim_in, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_in, dim_in, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_in, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                Downsample(dim_in, dim_out) if not is_last else Parallel(nn.Conv2d(dim_in, dim_out, 3, padding = 1), nn.Conv2d(dim_in, dim_out, 1)) # Downsample cross_embed_downsample CrossEmbedLayer(dim_in, dim_out, kernel_sizes = (2, 4))\n",
        "            ]))\n",
        "\n",
        "\n",
        "        mid_dim = dims[-1]\n",
        "        self.mid_block = Seq(\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "            TransformerBlock(mid_dim, depth = 1, n_heads=n_heads, d_head=d_head), # True # whether to have a layer of attention at the bottleneck (can turn off for higher resolution in cascading DDPM, before bringing in efficient attention)\n",
        "            ResnetBlock(mid_dim, mid_dim, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "        )\n",
        "\n",
        "        self.ups = nn.ModuleList([])\n",
        "        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):\n",
        "            is_last = ind == (num_layers - 1)\n",
        "            # skip_connect_dim = skip_connect_dims.pop()\n",
        "            # print('up', dim_in, dim_out, skip_connect_dim)\n",
        "            self.ups.append(nn.ModuleList([\n",
        "                ResnetBlock(dim_out + dim_in, dim_out, tok_dim = tok_dim, emb_dim = emb_dim, n_heads=n_heads, d_head=d_head),\n",
        "                nn.ModuleList([ResnetBlock(dim_out + dim_in, dim_out, emb_dim = emb_dim) for _ in range(0)]),\n",
        "                TransformerBlock(dim = dim_out, depth = 1, ff_mult = 2, context_dim = tok_dim, n_heads=n_heads, d_head=d_head), # trans/ lintrans/ id\n",
        "                PixelShuffleUpsample(dim_out, dim_in) if not is_last else nn.Identity() # PixelShuffleUpsample Upsample ; memory_efficient upscale at last too\n",
        "            ]))\n",
        "\n",
        "        self.final_res_block = ResnetBlock(d_model, d_model, emb_dim = emb_dim) #if final_resnet_block else None\n",
        "        self.final_conv = nn.Conv2d(d_model, self.out_ch, 3, padding = 3 // 2)\n",
        "        def zero_init_(m):\n",
        "            nn.init.zeros_(m.weight)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "        zero_init_(self.final_conv)\n",
        "\n",
        "    def forward(self, x, time, cond):\n",
        "        # cond_images = resize_image_to(cond_images, x.shape[-1], mode = 'nearest')\n",
        "        # x = torch.cat((cond_images, x), dim = 1)\n",
        "        x = self.init_conv(x)\n",
        "        # hiddens.append(x)\n",
        "\n",
        "        t, t_tok = self.time_mlp(time).split([self.emb_dim, self.num_time_tokens * self.tok_dim], dim=-1)\n",
        "        t_tok = t_tok.reshape(t_tok.shape[0], self.num_time_tokens, self.tok_dim)\n",
        "        cond_hid, c_tok = self.cond_mlp(cond).split([self.emb_dim, self.tok_dim], dim=-1)\n",
        "        # print('unet fwd', t_hid.shape, t_tok.shape, t.shape)\n",
        "\n",
        "        t = t + cond_hid # [batch, emb_dim]\n",
        "        c = torch.cat((t_tok, c_tok.unsqueeze(1)), dim=-2) # [b, num_toks, tok_dim]\n",
        "        c = self.norm_cond(c)\n",
        "\n",
        "        hiddens = []\n",
        "        # for pre_downsample, init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "        for init_block, resnet_blocks, attn_block, post_downsample in self.downs:\n",
        "            # x = pre_downsample(x)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = resnet_block(x, t)\n",
        "                hiddens.append(x)\n",
        "            x = attn_block(x, c)\n",
        "            hiddens.append(x)\n",
        "            x = post_downsample(x)\n",
        "\n",
        "        # print('unet fwd', x.shape, t.shape, c.shape)\n",
        "        x = self.mid_block(x, t, c)\n",
        "\n",
        "        for init_block, resnet_blocks, attn_block, upsample in self.ups:\n",
        "            x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "            x = init_block(x, t, c)\n",
        "            for resnet_block in resnet_blocks:\n",
        "                x = torch.cat((x, hiddens.pop() * 2**-.5), dim = 1)\n",
        "                x = resnet_block(x, t)\n",
        "            x = attn_block(x, c)\n",
        "            x = upsample(x)\n",
        "\n",
        "        # x = torch.cat((x, hiddens.pop()), dim = 1)\n",
        "        x = self.final_res_block(x, t)\n",
        "        return self.final_conv(x)\n",
        "\n",
        "model = UNet(d_model=128, c_dim=10, in_ch=1, dim_mults = (1, 2, 3, 4)).to(device)\n",
        "batch = 1\n",
        "# x = torch.rand((batch, 3, 64, 64), device = device)\n",
        "x = torch.rand((batch, 1,16,16), device = device)\n",
        "t = torch.rand(batch, device = device)\n",
        "# img_cond = torch.rand((batch, 512, 64, 64), device = device)\n",
        "cond = torch.rand((batch, 10), device = device)\n",
        "out = model(x, t, cond)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "print(out.shape)\n",
        "\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3) # 1e-3 3e-3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "1wCQ0gZ-Hz3i"
      },
      "outputs": [],
      "source": [
        "# @title mit-han-lab/efficientvit dc_ae.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/efficientvit/dc_ae.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def build_block(block_type, d_model, norm=None, act=None):\n",
        "    if block_type == \"ResBlock\": return ResBlock(d_model) # ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "    # ResBlock: bn2d, relu ; EViT_GLU: trms2d, silu\n",
        "    elif block_type == \"EViT_GLU\": return EfficientViTBlock(d_model) # EfficientViTBlock(d_model, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU:scales=() ; EViTS5_GLU sana:scales=(5,)\n",
        "\n",
        "class LevelBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, depth, block_type, norm=None, act=None, updown=None):\n",
        "        super().__init__()\n",
        "        stage = []\n",
        "        if updown=='up': stage.append(UpsampleBlock(in_ch, out_ch))\n",
        "        for d in range(depth):\n",
        "            # block = build_block(block_type=block_type, in_ch=d_model if d > 0 else in_ch, out_ch=d_model, norm=norm, act=act,)\n",
        "            block = build_block(block_type, out_ch if updown=='up' else in_ch, norm=norm, act=act,)\n",
        "            stage.append(block)\n",
        "        if updown=='down': stage.append(DownsampleBlock(in_ch, out_ch))\n",
        "        self.block = nn.Sequential(*stage)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "# stage = build_stage_main(width, depth, block_type)\n",
        "# downsample_block = DownsampleBlock(width, width_list[stage_id + 1])\n",
        "\n",
        "# upsample_block = UpsampleBlock(width_list[stage_id + 1], width)\n",
        "# stage.extend(build_stage_main(width, depth, block_type, \"bn2d\", \"silu\", input_width=width))\n",
        "\n",
        "\n",
        "class DownsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        # self.block = nn.Conv2d(in_ch, out_ch, 3, 2, 3//2)\n",
        "        self.block = ConvPixelUnshuffleDownSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = PixelUnshuffleChannelAveragingDownSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x):\n",
        "        # print(\"DownsampleBlock fwd\", x.shape, self.block(x).shape + self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,4,8,2,2,2]\n",
        "\n",
        "        # # self.project_in = nn.Conv2d(in_ch, width_list[0], 3, 1, 3//2) # if depth_list[0] > 0:\n",
        "        self.project_in = DownsampleBlock(in_ch, width_list[0]) # shortcut=None # self.project_in = ConvPixelUnshuffleDownSampleLayer(in_ch, width_list[0], kernel_size=3, r=2)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[0], width_list[-1], depth=depth_list[0], block_type='ResBlock', updown='down'),\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "        )\n",
        "\n",
        "        self.out_block = nn.Conv2d(width_list[-1], out_ch, 3, 1, 3//2)\n",
        "        self.out_shortcut = PixelUnshuffleChannelAveragingDownSampleLayer(width_list[-1], out_ch, r=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x = self.stages(x)\n",
        "        # print(\"Encoder fwd\", x.shape, self.out_block, self.out_shortcut(x).shape)\n",
        "        x = self.out_block(x) + self.out_shortcut(x)\n",
        "        return x\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.block = ConvPixelShuffleUpSampleLayer(in_ch, out_ch, kernel_size=3, r=2)\n",
        "        # self.block = InterpolateConvUpSampleLayer(in_ch=in_ch, out_ch=out_ch, kernel_size=3, r=2)\n",
        "        self.shortcut_block = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, out_ch, r=2)\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,o,2h,2w]\n",
        "        print(\"UpsampleBlock fwd\", x.shape, self.block(x).shape, self.shortcut_block(x).shape)\n",
        "        return self.block(x) + self.shortcut_block(x)\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        width_list=[d_model*m for m in mult]\n",
        "        # mult=[1,2,4,4,8,8]\n",
        "        # depth_list=[0,5,10,2,2,2]\n",
        "\n",
        "        self.in_block = nn.Conv2d(in_ch, width_list[-1], 3, 1, 3//2)\n",
        "        self.in_shortcut = ChannelDuplicatingPixelUnshuffleUpSampleLayer(in_ch, width_list[-1], r=1)\n",
        "\n",
        "        self.stages = nn.Sequential(\n",
        "            LevelBlock(width_list[-1], width_list[-1], depth=depth_list[-1], block_type='EViT_GLU', updown=None),\n",
        "            LevelBlock(width_list[-1], width_list[0], depth=depth_list[0], block_type='ResBlock', updown='up'),\n",
        "        )\n",
        "\n",
        "        # if depth_list[0] > 0:\n",
        "        # self.project_out = nn.Sequential(\n",
        "        #     nn.BatchNorm2d(width_list[0]), nn.ReLU(), nn.Conv2d(width_list[0], out_ch, 3, 1, 3//2) # norm=\"trms2d\"\n",
        "        #     )\n",
        "        # else:\n",
        "        self.project_out = nn.Sequential(\n",
        "            nn.BatchNorm2d(width_list[0]), nn.ReLU(), UpsampleBlock(width_list[0], out_ch) # shortcut=None ; norm=\"trms2d\"\n",
        "            # nn.BatchNorm2d(width_list[0]), nn.ReLU(), ConvPixelShuffleUpSampleLayer(width_list[0], out_ch, kernel_size=3, r=2) # shortcut=None ; norm=\"trms2d\"\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.in_block(x) + self.in_shortcut(x)\n",
        "        x = self.stages(x)\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DCAE(nn.Module):\n",
        "    def __init__(self, in_ch=3, out_ch=4, d_model=16, mult=[1], depth_list=[1,1]):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(in_ch, out_ch, d_model, mult, depth_list)\n",
        "        self.decoder = Decoder(out_ch, in_ch, d_model, mult, depth_list)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        print(x.shape)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "# https://discuss.pytorch.org/t/is-there-a-layer-normalization-for-conv2d/7595/5\n",
        "# https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html\n",
        "\n",
        "in_ch=3\n",
        "out_ch=3\n",
        "# 3*2^2|d_model\n",
        "model = DCAE(in_ch, out_ch, d_model=24, mult=[1,1], depth_list=[1,1]).to(device)\n",
        "# model = Encoder(in_ch, out_ch, d_model=32, mult=[1,1], depth_list=[2,2])\n",
        "# print(sum(p.numel() for p in model.project_in.parameters() if p.requires_grad)) # 896\n",
        "# print(sum(p.numel() for p in model.stages.parameters() if p.requires_grad)) # 4393984\n",
        "# print(sum(p.numel() for p in model.out_shortcut.parameters() if p.requires_grad)) # 0\n",
        "# print(sum(p.numel() for p in model.out_block.parameters() if p.requires_grad)) # 18436\n",
        "# model = Decoder(out_ch, in_ch)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 19683\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "x = torch.rand((2,in_ch,64,64), device=device)\n",
        "out = model(x)\n",
        "print(out.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "JYMQDoL578HQ"
      },
      "outputs": [],
      "source": [
        "# @title efficientvit nn/ops.py down\n",
        "# https://github.com/mit-han-lab/efficientvit/blob/master/efficientvit/models/nn/ops.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ConvLayer\n",
        "# nn.Sequential(\n",
        "#     nn.Dropout2d(dropout), nn.Conv2d(in_ch, out_ch, 3, 1, 3//2, bias=False), nn.BatchNorm2d(out_ch), nn.ReLU()\n",
        "# )\n",
        "\n",
        "class ConvPixelUnshuffleDownSampleLayer(nn.Module): # down main [b,i,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch//r**2, kernel_size, 1, kernel_size//2)\n",
        "\n",
        "    def forward(self, x): # [b,i,2h,2w] -> [b,o/4,2h,2w] -> [b,o,h,w]\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class PixelUnshuffleChannelAveragingDownSampleLayer(nn.Module): # down shortcut [b,c,2h,2w] -> [b,o,h,w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch*r**2, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,2h,2w] -> [b,4c,h,w] -> [b,o,4c/o,h,w] -> [b,o,h,w]\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "        return x\n",
        "\n",
        "class ChannelDuplicatingPixelUnshuffleUpSampleLayer(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PixelShortcut(nn.Module): # up shortcut [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.samech = SameCh(in_ch, out_ch*r**2)\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale r r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        # down\n",
        "        x = F.pixel_unshuffle(x, self.r)\n",
        "        x = self.samech(x)\n",
        "\n",
        "        # up\n",
        "        x = self.samech(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "class SameCh(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch):\n",
        "        super().__init__()\n",
        "        self.repeats = out_ch//in_ch\n",
        "        if out_ch//in_ch > 1:\n",
        "            self.func = lambda x: x.repeat_interleave(out_ch//in_ch, dim=1) # [b,i,h,w] -> [b,o,h,w]\n",
        "        elif in_ch//out_ch > 1:\n",
        "            self.func = lambda x: torch.unflatten(x, 1, (out_ch, in_ch//out_ch)).mean(dim=2) # [b,i,h,w] -> [b,o,i/o,h,w] -> [b,o,h,w]\n",
        "        else: print('err SameCh', in_ch, out_ch)\n",
        "\n",
        "    def forward(self, x): # [b,c,h,w] -> [b,c * 4o/c,h,w] -> [b,o,2h,2w]\n",
        "        return self.func(x)\n",
        "\n",
        "\n",
        "class ConvPixelShuffleUpSampleLayer(nn.Module): # up main [b,c,h,w] -> [b,o,2h,2w]\n",
        "    def __init__(self, in_ch, out_ch, kernel_size, r):\n",
        "        super().__init__()\n",
        "        self.r = r\n",
        "        self.conv = nn.Conv2d(in_ch, out_ch*r**2, kernel_size, 1, kernel_size//2)\n",
        "        # self.conv = nn.Conv2d(in_ch, out_ch, kernel_size, 1, kernel_size//2) # InterpolateConvUpSampleLayer\n",
        "\n",
        "    def forward(self, x): # [b,i,h,w] -> [b,4o,h,w] -> [b,o,2h,2w]\n",
        "        # x = torch.nn.functional.interpolate(x, scale_r=self.r, mode=\"nearest\")\n",
        "        x = self.conv(x)\n",
        "        x = F.pixel_shuffle(x, self.r)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "# block = EfficientViTBlock(in_ch, norm=norm, act_func=act, local_module=\"GLUMBConv\", scales=()) # EViT_GLU\n",
        "# self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio,\n",
        "#     use_bias=(True, True, False), norm=(None, None, norm), act_func=(act_func, act_func, None))\n",
        "class GLUMBConv(nn.Module):\n",
        "    # def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4, use_bias=False, norm=(None, None, \"ln2d\"), act_func=(\"silu\", \"silu\", None)):\n",
        "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, mid_channels=None, expand_ratio=4):\n",
        "        super().__init__()\n",
        "        mid_channels = round(in_ch * expand_ratio) if mid_channels is None else mid_channels\n",
        "        # self.glu_act = build_act(act_func[1], inplace=False)\n",
        "        # self.inverted_conv = ConvLayer(in_ch, mid_channels * 2, 1, use_bias=use_bias[0], norm=norm[0], act_func=act_func[0],)\n",
        "        # self.depth_conv = ConvLayer(mid_channels * 2, mid_channels * 2, kernel_size, stride=stride, groups=mid_channels * 2, use_bias=use_bias[1], norm=norm[1], act_func=None,)\n",
        "        self.inverted_depth_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, mid_channels*2, 1, 1, 0), nn.SiLU(),\n",
        "            nn.Conv2d(mid_channels*2, mid_channels*2, 3, 1, 3//2, groups=mid_channels*2),\n",
        "        )\n",
        "        # self.point_conv = ConvLayer(mid_channels, out_ch, 1, use_bias=use_bias[2], norm=norm[2], act_func=act_func[2],)\n",
        "        self.point_conv = nn.Sequential(\n",
        "            nn.Conv2d(mid_channels, out_ch, 1, 1, 0, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = self.inverted_conv(x)\n",
        "        # x = self.depth_conv(x)\n",
        "        x = self.inverted_depth_conv(x)\n",
        "        x, gate = torch.chunk(x, 2, dim=1)\n",
        "        x = x * nn.SiLU()(gate)\n",
        "        x = self.point_conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# main_block = ResBlock(in_ch=in_ch, out_ch=out_ch, kernel_size=3, stride=1, use_bias=(True, False), norm=(None, bn2d), act_func=(relu/silu, None),)\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch=None, kernel_size=3, stride=1, d_model=None,\n",
        "        use_bias=False, norm=(\"bn2d\", \"bn2d\"), act_func=(\"relu6\", None)):\n",
        "        super().__init__()\n",
        "        d_model = d_model or in_ch\n",
        "        out_ch = out_ch or in_ch\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, d_model, kernel_size, stride, kernel_size//2), nn.SiLU(),\n",
        "            nn.Conv2d(d_model, out_ch, kernel_size, 1, kernel_size//2, bias=False), nn.BatchNorm2d(out_ch),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.conv(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class EfficientViTBlock(nn.Module):\n",
        "    def __init__(self, in_ch, heads_ratio = 1.0, dim=32, expand_ratio=1, # expand_ratio=4\n",
        "        # scales: tuple[int, ...] = (5,), # (5,): sana\n",
        "        # act_func = \"hswish\", # nn.Hardswish()\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # self.context_module = LiteMLA(in_ch, in_ch, heads_ratio=heads_ratio, dim=dim, norm=(None, norm), scales=scales,)\n",
        "        self.context_module = AttentionBlock(in_ch, d_head=8)\n",
        "        # self.local_module = MBConv(\n",
        "        self.local_module = GLUMBConv(in_ch, in_ch, expand_ratio=expand_ratio)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.context_module(x)\n",
        "        x = x + self.local_module(x)\n",
        "        return x\n",
        "\n",
        "# class ResidualBlock(nn.Module):\n",
        "    # def forward(self, x):\n",
        "    #     res = self.forward_main(self.pre_norm(x)) + self.shortcut(x)\n",
        "    #     res = self.post_act(res)\n",
        "    #     return res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "r-cYioQer02v"
      },
      "outputs": [],
      "source": [
        "# @title CrossEmbedLayer PixelShuffleConv\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "class CrossEmbedLayer(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, kernel_sizes, stride=1):\n",
        "        super().__init__()\n",
        "        kernel_sizes = sorted(kernel_sizes)\n",
        "\n",
        "        r=2\n",
        "        balls = out_ch//r**2\n",
        "        mult = [1/(1.6**i) for i in range(len(kernel_sizes))]\n",
        "        mul = balls/sum(mult)\n",
        "        mult = [m*mul for m in mult]\n",
        "        dim_scales = [0]*len(kernel_sizes)\n",
        "        for i in range(balls):\n",
        "            ind = mult.index(max(mult))\n",
        "            dim_scales[ind] += 1\n",
        "            mult[ind] -= 1\n",
        "        dim_scales = [d*r**2 for d in dim_scales]\n",
        "        if 0 in dim_scales: print('dim_scales',dim_scales)\n",
        "        # 1/2 + 1/4 + 1/8 + ... + 1/2^num_kernels + 1/2^num_kernels of out_ch; smaller kernel allocated more channels\n",
        "        self.convs = nn.ModuleList([nn.Conv2d(in_ch, dim_scale, kernel, stride=stride, padding=(kernel-stride)//2) for kernel, dim_scale in zip(kernel_sizes, dim_scales)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        # return torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        out = torch.cat([conv(x) for conv in self.convs], dim = 1)\n",
        "        b,c,h,w = out.shape\n",
        "        out = out.reshape(b, -1, 4, h, w).transpose(1,2).reshape(b, c, h, w)\n",
        "        return out\n",
        "\n",
        "\n",
        "class PixelShuffleConv(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch = None, kernel_size=3, r=2):\n",
        "        super().__init__()\n",
        "        if out_ch==None: out_ch = in_ch\n",
        "        self.in_ch, self.out_ch, self.r = in_ch, out_ch, r\n",
        "        r = max(r, int(1/r))\n",
        "        if self.r>1: self.net = nn.Sequential(nn.Conv2d(in_ch, out_ch * r**2, kernel_size, 1, padding=kernel_size//2), nn.PixelShuffle(r)) # PixelShuffle: [b,c*r^2,h,w] -> [b,c,h*r,w*r] # upscale by upscale factor r # https://arxiv.org/pdf/1609.05158v2\n",
        "        elif self.r<1: self.net = nn.Sequential(nn.PixelUnshuffle(r), nn.Conv2d(in_ch * r**2, out_ch, kernel_size, 1, padding=kernel_size//2)) # PixelUnshuffle: [b,c,h*r,w*r] -> [b,c*r^2,h,w]\n",
        "        self.net.apply(self.init_conv_)\n",
        "\n",
        "    def init_conv_(self, conv): # weight initialisation very important for the performance of pixelshuffle!\n",
        "        if isinstance(conv, nn.Conv2d):\n",
        "            o, i, h, w = conv.weight.shape\n",
        "            conv_weight = torch.empty(self.out_ch, self.in_ch, h, w)\n",
        "            nn.init.kaiming_uniform_(conv_weight)\n",
        "            # print(conv.weight.shape, conv_weight.shape,max(self.r, int(1/self.r)), (0 if self.r>1 else 1))\n",
        "            conv.weight.data.copy_(conv_weight.repeat_interleave(max(self.r, int(1/self.r))**2, dim=(0 if self.r>1 else 1)))\n",
        "            nn.init.zeros_(conv.bias.data)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# d=PixelShuffleConv(3, 16, 7, r=1/2)\n",
        "\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# model = PixelShuffleConv(in_ch=3, r=2).to(device)\n",
        "# print(sum(p.numel() for p in model.parameters() if p.requires_grad)) # 16x16 conv 17651 ; pixel(3)(3)  ; (1)(1)  ; (3,7,15)(3,7)  ; (3,5,7)(3,5) 42706 ; 7,5 70226\n",
        "# input = torch.rand((4,3,64,64), device=device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "# model = PixelShuffleConv(in_ch=3, r=1/2).to(device)\n",
        "# out = model(input)\n",
        "# print(out.shape)\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}